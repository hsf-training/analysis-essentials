{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import uproot\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "\n",
    "from sklearn import cross_validation, metrics\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "from sklearn.model_selection import KFold, cross_validate, cross_val_score\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# This gives us a special function for this lesson that lets you check how good your selection is\n",
    "from python_lesson import check_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "source": [
    "def plot_mass(df):\n",
    "    counts, bins, _ = plt.hist(df['Jpsi_M'], bins=100, range=[2.75, 3.5], histtype='step')\n",
    "    # You can also use LaTeX in the axis label\n",
    "    plt.xlabel('$J/\\\\psi$ mass [GeV]')\n",
    "    plt.xlim(bins[0], bins[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "source": [
    "def plot_comparision(var, mc_df, bkg_df):\n",
    "    _, bins, _ = plt.hist(mc_df[var], bins=100, histtype='step', label='MC', density=1)\n",
    "    _, bins, _ = plt.hist(bkg_df[var], bins=bins, histtype='step', label='Background', density=1)\n",
    "    plt.xlabel(var)\n",
    "    plt.xlim(bins[0], bins[-1])\n",
    "    plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "source": [
    "def plot_roc(bdt, training_data, training_columns, label=None):\n",
    "    y_score = bdt.predict_proba(training_data[training_columns])[:,1]\n",
    "    fpr, tpr, thresholds = roc_curve(training_data['catagory'], y_score)\n",
    "    area = auc(fpr, tpr)\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], color='grey', linestyle='--')\n",
    "    if label:\n",
    "        plt.plot(fpr, tpr, label=f'{label} (area = {area:.2f})')\n",
    "    else:\n",
    "        plt.plot(fpr, tpr, label=f'ROC curve (area = {area:.2f})')\n",
    "    plt.xlim(0.0, 1.0)\n",
    "    plt.ylim(0.0, 1.0)\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.legend(loc='lower right')\n",
    "    # We can make the plot look nicer by forcing the grid to be square\n",
    "    plt.gca().set_aspect('equal', adjustable='box')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "source": [
    "def plot_significance(bdt, training_data, training_columns, label=None):\n",
    "    y_score = bdt.predict_proba(training_data[training_columns])[:,1]\n",
    "    fpr, tpr, thresholds = roc_curve(training_data['catagory'], y_score)\n",
    "\n",
    "    n_sig = 1200\n",
    "    n_bkg = 23000\n",
    "    S = n_sig*tpr\n",
    "    B = n_bkg*fpr\n",
    "    metric = S/np.sqrt(S+B)\n",
    "\n",
    "    plt.plot(thresholds, metric, label=label)\n",
    "    plt.xlabel('BDT cut value')\n",
    "    plt.ylabel('$\\\\frac{S}{\\\\sqrt{S+B}}$')\n",
    "    plt.xlim(0, 1.0)\n",
    "\n",
    "    optimal_cut = thresholds[np.argmax(metric)]\n",
    "    plt.axvline(optimal_cut, color='black', linestyle='--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "source": [
    "#max_entries = 1000\n",
    "data_df = uproot.open('/eos/user/l/lhcbsk/advanced-python/data/real_data.root')['DecayTree'].pandas.df()#entrystop=max_entries)\n",
    "mc_df = uproot.open('/eos/user/l/lhcbsk/advanced-python/data/simulated_data.root')['DecayTree'].pandas.df()#entrystop=max_entries)\n",
    "bkg_df = data_df.query('~(3.0 < Jpsi_M < 3.2)').copy()\n",
    "\n",
    "for df in [mc_df, data_df, bkg_df]:\n",
    "    df.eval('Jpsi_eta = arctanh(Jpsi_PZ/Jpsi_P)', inplace=True)\n",
    "    df.eval('mup_P = sqrt(mum_PX**2 + mum_PY**2 + mum_PZ**2)', inplace=True)\n",
    "    df.eval('mum_P = sqrt(mum_PX**2 + mum_PY**2 + mum_PZ**2)', inplace=True)\n",
    "\n",
    "bkg_df['catagory'] = 0  # Use 0 for background\n",
    "mc_df['catagory'] = 1  # Use 1 for signal\n",
    "training_data = pd.concat([bkg_df, mc_df], copy=True, ignore_index=True)\n",
    "for df in [mc_df, bkg_df, data_df, training_data]:\n",
    "    df['IPdiff'] = np.abs(df['mum_PT'] - df['mup_PT'])\n",
    "                          \n",
    "training_columns = [\n",
    "    'Jpsi_PT',\n",
    "    'mup_PT', 'mup_eta', 'mup_ProbNNmu', 'mup_IP',\n",
    "    'mum_PT', 'mum_eta', 'mum_ProbNNmu', 'mum_IP',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $k$-folding\n",
    "\n",
    "Let's go search for `scikit learn k-folding`.\n",
    "\n",
    " - https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html\n",
    "\n",
    "Look at the example section:\n",
    "\n",
    "```python\n",
    ">>> from sklearn.model_selection import KFold\n",
    ">>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n",
    ">>> y = np.array([1, 2, 3, 4])\n",
    ">>> kf = KFold(n_splits=2)\n",
    ">>> kf.get_n_splits(X)\n",
    "2\n",
    ">>> print(kf)  \n",
    "KFold(n_splits=2, random_state=None, shuffle=False)\n",
    ">>> for train_index, test_index in kf.split(X):\n",
    "...    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "...    X_train, X_test = X[train_index], X[test_index]\n",
    "...    y_train, y_test = y[train_index], y[test_index]\n",
    "TRAIN: [2 3] TEST: [0 1]\n",
    "TRAIN: [0 1] TEST: [2 3]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "source": [
    "X1, y1 = training_data[training_columns], training_data['catagory']\n",
    "splits = 5\n",
    "kf = KFold(splits,True)\n",
    "for train, test in kf.split(X1):\n",
    "    X_train, X_test = X1.iloc[train], X1.iloc[test]\n",
    "    y_train, y_test = y1.iloc[train], y1.iloc[test]\n",
    "    xgboost_bdt.fit(X_train,y_train)\n",
    "cv_acc_1 = cross_val_score(xgboost_bdt, X_test, y_test, cv=splits, scoring=\"accuracy\")\n",
    "cv_los_1 = cross_val_score(xgboost_bdt, X_test, y_test, cv=splits, scoring=\"neg_log_loss\")\n",
    "cv_auc_1 = cross_val_score(xgboost_bdt, X_test, y_test, cv=splits, scoring=\"roc_auc\")\n",
    "print(\"accuracy: \",cv_acc_1)\n",
    "print(\"-logloss: \",cv_los_1)\n",
    "print(\"roc_auc:  \",cv_auc_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "source": [
    "def modelfit(alg, metric, train, test, predictors, cv_folds=5, early_stop=10): #50):\n",
    "    xgb_param = alg.get_xgb_params()\n",
    "    xgtrain = xgb.DMatrix(train, label=test, feature_names=predictors)\n",
    "    cvresult = xgb.cv(xgb_param,\n",
    "                      xgtrain,\n",
    "                      num_boost_round=alg.get_params()['n_estimators'],\n",
    "                      nfold=cv_folds,\n",
    "                      metrics=metric,\n",
    "                      early_stopping_rounds=early_stop)\n",
    "    alg.set_params(n_estimators=cvresult.shape[0])\n",
    "    #Fit the algorithm on the data                                                                                                                                                                     \n",
    "    alg.fit(train, test, eval_metric=metric)\n",
    "    #Predict training set:                                                                                                                                                                             \n",
    "    train_predictions = alg.predict(train)\n",
    "    #Print model report:                                                                                                                                                                               \n",
    "    print(\"\\nModel Report : best iteration \"+str(cvresult.shape[0]))\n",
    "    print(\"Accuracy : \"+str(metrics.accuracy_score(test, train_predictions)))\n",
    "    return cvresult.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "source": [
    "X1, y1 = training_data[training_columns], training_data['catagory']\n",
    "LR = 0.2 # choosing a high learning rate to establish earlystopping limit to use during grid scan\n",
    "bdt0 = XGBClassifier(   learning_rate=LR,      n_estimators=1000,\n",
    "                        #max_depth=6,           min_child_weight=1, #default values\n",
    "                        #gamma=0,              subsample=0.8,\n",
    "                        #colsample_bytree=0.8, scale_pos_weight=1,\n",
    "                        objective='binary:logistic', #'mutli:softprob', num_class=3, #or more\n",
    "                        seed=123)\n",
    "estimators = modelfit(bdt0, 'error', X1, y1, training_columns) #'merror' for multiclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "source": [
    "bdt1 = XGBClassifier( learning_rate=LR,     n_estimators=estimators,\n",
    "                      #max_depth=6,          min_child_weight=1,  #default values\n",
    "                      #gamma=0,              subsample=0.8,\n",
    "                      #colsample_bytree=0.8, scale_pos_weight=1,  \n",
    "                      objective='binary:logistic', #'mutli:softprob', num_class=3, #or more\n",
    "                      seed=123)\n",
    "    \n",
    "param_test1 = {\n",
    "    'max_depth':np.arange(        5,   9,   2   ),\n",
    "    'min_child_weight':np.arange( 1,   5,   2   ),\n",
    "    #'gamma':np.arange(            0.0, 1.0, 0.2 ),\n",
    "    #'colsample_bytree':np.arange( 0.4, 1.0, 0.2 ),\n",
    "    #'subsample':np.arange(        0.4, 1.0, 0.2 ),\n",
    "    #'scale_pos_weight':np.arange( 0.4, 1.6, 0.2 )\n",
    "}\n",
    "gsearch1 = GridSearchCV(estimator=bdt1,\n",
    "                        param_grid=param_test1,\n",
    "                        scoring='accuracy',\n",
    "                        iid=False,\n",
    "                        cv=5)\n",
    "gsearch1.fit(X1,y1)\n",
    "print(gsearch1.best_params_)\n",
    "print(gsearch1.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "source": [
    "#second stage with decreased step size and smaller grid scan\n",
    "bdt2 = XGBClassifier( learning_rate=LR,      n_estimators=estimators,\n",
    "                      #max_depth=6,           min_child_weight=1,  #default values\n",
    "                      #gamma=0,              subsample=0.8,\n",
    "                      #colsample_bytree=0.8, scale_pos_weight=1,\n",
    "                      objective='binary:logistic', #'mutli:softprob', num_class=3, #or more\n",
    "                      seed=123)\n",
    "param_test2 = {\n",
    "            'max_depth':np.arange( gsearch1.best_params_['max_depth']-1 if gsearch1.best_params_['max_depth']>=4 else gsearch1.best_params_['max_depth'],\n",
    "                                   gsearch1.best_params_['max_depth']+1, 1 ),\n",
    "     'min_child_weight':np.arange( gsearch1.best_params_['min_child_weight']-1 if gsearch1.best_params_['min_child_weight']>=1.1 else gsearch1.best_params_['min_child_weight'],\n",
    "                                   gsearch1.best_params_['min_child_weight']+1, 1 ), #0.5 ),\n",
    "    #           'gamma':np.arange( gsearch1.best_params_['gamma']-0.1 if gsearch1.best_params_['gamma']>=0.1 else gsearch1.best_params_['gamma'],\n",
    "    #                              gsearch1.best_params_['gamma']+0.1, 0.05 ),\n",
    "    #'colsample_bytree':np.arange( gsearch1.best_params_['colsample_bytree']-0.1 if gsearch1.best_params_['colsample_bytree']>=1.1 else gsearch1.best_params_['colsample_bytree'],\n",
    "    #                              gsearch1.best_params_['colsample_bytree']+0.1, 0.05 ),\n",
    "    #       'subsample':np.arange( gsearch1.best_params_['subsample']-0.1 if gsearch1.best_params_['subsample']>=1.1 else gsearch1.best_params_['subsample'],\n",
    "    #                              gsearch1.best_params_['subsample']+0.1, 0.05 ),\n",
    "    #'scale_pos_weight':np.arange( gsearch1.best_params_['scale_pos_weight']-0.1 if gsearch1.best_params_['scale_pos_weight']>=1.1 else gsearch1.best_params_['scale_pos_weight'],\n",
    "    #                              gsearch1.best_params_['scale_pos_weight']+0.1, 0.05 )\n",
    "}\n",
    "gsearch2 = GridSearchCV(estimator=bdt2,\n",
    "                        param_grid=param_test2,\n",
    "                        scoring='accuracy',\n",
    "                        iid=False,\n",
    "                        cv=5)\n",
    "gsearch2.fit(X1,y1)\n",
    "print(gsearch2.best_params_)\n",
    "print(gsearch2.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "source": [
    "# now repeat with lower learning rate, early stopping monitoring using optimal hyperparameters\n",
    "bdt3 = XGBClassifier( learning_rate=0.1, n_estimators=1000, # 0.1 learning rate to compare to default used in xgboost_bdt\n",
    "                      max_depth=gsearch2.best_params_['max_depth'], min_child_weight=gsearch2.best_params_['min_child_weight'],\n",
    "                      #gamma=gsearch2.best_params_['gamma'], subsample=gsearch2.best_params_['subsample'],\n",
    "                      #colsample_bytree=gsearch2.best_params_['colsample_bytree'], scale_pos_weight=gsearch2.best_params_['scale_pos_weight'],  \n",
    "                      objective='binary:logistic', #'multi:softprob', num_class=3, #or more\n",
    "                      seed=123 )\n",
    "estimators = modelfit(bdt3, 'error', X1, y1, training_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# now repeat with lower learning rate, early stopping monitoring using optimal hyperparameters\n",
    "bdt4 = XGBClassifier( learning_rate=0.01, n_estimators=10000, # even lower learning rate to compare to default\n",
    "                      max_depth=gsearch2.best_params_['max_depth'], min_child_weight=gsearch2.best_params_['min_child_weight'],\n",
    "                      #gamma=gsearch2.best_params_['gamma'], subsample=gsearch2.best_params_['subsample'],\n",
    "                      #colsample_bytree=gsearch2.best_params_['colsample_bytree'], scale_pos_weight=gsearch2.best_params_['scale_pos_weight'],  \n",
    "                      objective='binary:logistic', #'multi:softprob', num_class=3, #or more\n",
    "                      seed=123 )\n",
    "estimators = modelfit(bdt4, 'error', X1, y1, training_columns)\n",
    "\n",
    "bdt4.fit(training_data[training_columns], training_data['catagory'])\n",
    "for df in [mc_df, bkg_df, data_df, training_data]:\n",
    "    df['XGBtune'] = bdt4.predict_proba(df[training_columns])[:,1]\n",
    "\n",
    "plt.figure()\n",
    "plot_comparision('XGBtune', mc_df, bkg_df)\n",
    "\n",
    "plt.figure()\n",
    "plot_significance(bdt4, training_data, training_columns)\n",
    "\n",
    "plt.figure()\n",
    "plot_roc(bdt4, training_data, training_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
