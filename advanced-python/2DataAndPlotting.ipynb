{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2: First look at data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson we will look at a toy dataset simulating $J/\\psi \\rightarrow \\mu^+ \\mu^-$ events. We will discuss ways of loading the data in python, data formats and plotting with ```matplotlib```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recap: Importing modules\n",
    "\n",
    "It's generally seen as good practice to put imports at the top of your file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import uproot\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. The toy dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to look at some fake $J/\\psi \\rightarrow \\mu^+ \\mu^-$ data, the variables available are:\n",
    "\n",
    "- `Jpsi_M` `Jpsi_P` `Jpsi_PT` `Jpsi_PE` `Jpsi_PX` `Jpsi_PY` `Jpsi_PZ`\n",
    "- `mum_M` `mum_PT` `mum_eta` `mum_PE` `mum_PX` `mum_PY` `mum_PZ` `mum_IP` `mum_ProbNNmu` `mum_ProbNNpi`\n",
    "- `mup_M` `mup_PT` `mup_eta` `mup_PE` `mup_PX` `mup_PY` `mup_PZ` `mup_IP` `mup_ProbNNmu` `mup_ProbNNpi`\n",
    "- `nTracks`\n",
    "\n",
    "The meanings of the suffixes are as follows:\n",
    "\n",
    "- `_M`: Invarient mass of the particle (fixed to the PDG value for muons)\n",
    "- `_P`: Absolute value of the particle's three momentum\n",
    "- `_PT`: Absolute value of the particle's momentum in the `x`-`y` plane\n",
    "- `_PE`, `_PX`, `_PY`, `_PZ`: Four momentum of the particle\n",
    "- `_IP`: Impact parameter, i.e. the distance of closest approach between the reconstructed particle and the primary vertex\n",
    "- `ProbNNmu`, `ProbNNpi`: Particle identificaton variables which corrospond to how likely is it that the particle is really a muon or a pion\n",
    "- `nTracks`: The total number of tracks in the event"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data\n",
    "\n",
    " - `root_numpy` and `root_pandas` are a way of reading+writing ROOT files\n",
    " - `uproot` is a way of reading ROOT files without having ROOT installed, see the github reposity [here](https://github.com/scikit-hep/uproot)\n",
    " - We can look at the objects that are available in the file and access objects using dictionary style syntax\n",
    " - The tree class contains converters to a varity of common Python libraries, such as numpy\n",
    " - We will also use `pandas DataFrames` to load data in a table like format\n",
    "\n",
    "First let's load the data using `uproot`.\n",
    "\n",
    "Often it is convenient to access data stored on the *grid* at CERN so you don't have to keep it locally. This can be done using the *XRootD* protocol:\n",
    "\n",
    "```python\n",
    "my_file = uproot.open('root://eosuser.cern.ch//eos/user/l/lhcbsk/advanced-python/data/real_data.root')\n",
    "```\n",
    "\n",
    "Accessing data this way requires you to have valid CERN credentials to access it. If authenication fails you will see an error message like:\n",
    "\n",
    "```\n",
    "OSError: [ERROR] Server responded with an error: [3010] Unable to give access - user access restricted - unauthorized identity used ; Permission denied\n",
    "```\n",
    "\n",
    "Credentials can be obtained by typing `kinit username@CERN.CH` in your terminal and entering your CERN password.\n",
    "\n",
    "For this tutorial we will use a publically accessible file instead, using HTTPS to access it remotely. This is significantly slower then using XRootD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_file = uproot.open('https://cern.ch/starterkit/data/advanced-python-2018/real_data.root',\n",
    "                      httpsource={'chunkbytes': 1024*1024, 'limitbytes': 33554432, 'parallel': 64})\n",
    "my_file.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = my_file['DecayTree']\n",
    "# Get a numpy array containing the J/Î¨ mass\n",
    "tree.array('Jpsi_M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data as a pandas DataFrame\n",
    "data_df = tree.pandas.df()\n",
    "# Show the first 5 lines of the DataFrame\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Plotting a histogram with `matplotlib`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with a basic histogram\n",
    "plt.hist(data_df['Jpsi_M'])\n",
    "plt.xlabel('Jpsi mass')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's okay but we could use some more bins, lets make it tidier and turn it into a function we can use later.\n",
    "\n",
    "Take a look at the `matplotlib` documentation:\n",
    " - https://matplotlib.org/api/_as_gen/matplotlib.pyplot.hist.html\n",
    " - It returns an array of counts, an array of bins and an array of patches. We don't care about the patches so we put them into a junk variable `_`.\n",
    " - Lets also set `histtype=\"step\"` so we can plot multiple datasets on the same axis easily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mass(df):\n",
    "    counts, bins, _ = plt.hist(df['Jpsi_M'], bins=100, range=[2.75, 3.5], histtype='step')\n",
    "    # You can also use LaTeX in the axis label\n",
    "    plt.xlabel('$J/\\\\psi$ mass [GeV]')\n",
    "    plt.xlim(bins[0], bins[-1])\n",
    "\n",
    "plot_mass(data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When making the ROOT file we forgot to add some variables, no bother lets add them now!\n",
    "data_df.eval('Jpsi_eta = arctanh(Jpsi_PZ/Jpsi_P)', inplace=True)\n",
    "data_df.head()['Jpsi_eta']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Add `mu_P` and `mum_P` columns to the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.eval('mup_P = sqrt(mup_PX**2 + mup_PY**2 + mup_PZ**2)', inplace=True)\n",
    "data_df.eval('mum_P = sqrt(mum_PX**2 + mum_PY**2 + mum_PZ**2)', inplace=True)\n",
    "# We can also get multiple columns at the same time\n",
    "data_df.head()[['mum_P', 'mup_P']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using rectangular cuts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We want to increase the 'signal significance' of our sample - this means more signal events with respect to background\n",
    "* To do this we can cut on certain discriminating variables\n",
    "* Here we will make cuts on the `Jpsi_PT` and **PID** (Particle Identification) variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mass(data_df)\n",
    "data_with_cuts_df = data_df.query('Jpsi_PT > 4')\n",
    "plot_mass(data_with_cuts_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mass(data_df)\n",
    "data_with_cuts_df = data_df.query('Jpsi_PT > 4')\n",
    "plot_mass(data_with_cuts_df)\n",
    "# Lets add some PID cuts as well\n",
    "data_with_cuts_df = data_df.query('(Jpsi_PT > 4) & ((mum_ProbNNmu > 0.9) & (mup_ProbNNmu > 0.9))')\n",
    "plot_mass(data_with_cuts_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go back and add a label argument to our plot function. This makes it easier to identify each line.\n",
    "We can also use the `density` argument in `matplotlib.hist` to plot all the histograms as the same scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mass(df, **kwargs):\n",
    "    counts, bins, _ = plt.hist(df['Jpsi_M'], bins=100, range=[2.75, 3.5], histtype='step', **kwargs)\n",
    "    plt.xlabel('$J/\\\\psi$ mass [GeV]')\n",
    "    plt.xlim(bins[0], bins[-1])\n",
    "\n",
    "plot_mass(data_df, label='No cuts', density=1)\n",
    "data_with_cuts_df = data_df.query('Jpsi_PT > 4')\n",
    "plot_mass(data_with_cuts_df, label='$J/\\\\psi$ p$_T$ only', density=1)\n",
    "data_with_cuts_df = data_df.query('(Jpsi_PT > 4) & ((mum_ProbNNmu > 0.9) & (mup_ProbNNmu > 0.9))')\n",
    "plot_mass(data_with_cuts_df, label='$J/\\\\psi$ p$_T$ and muon PID', density=1)\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this tutorial we have a special function for testing the significance of the signal in our dataset. There are many different ways to do this with real data, though we will not cover them here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Work out what check_truth function is.\n",
    "data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Originally the significance is')\n",
    "#check_truth(data_df)\n",
    "\n",
    "print('\\nCutting on pT gives us')\n",
    "#check_truth(data_df.query('Jpsi_PT > 4'))\n",
    "\n",
    "print('\\nCutting on pT and ProbNNmu gives us')\n",
    "#check_truth(data_df.query('(Jpsi_PT > 4) & ((mum_ProbNNmu > 0.9) & (mup_ProbNNmu > 0.9))'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Comparing distributions\n",
    "\n",
    "Before we just used the cuts that were told to you but how do we pick them?\n",
    "\n",
    "One way is to get a sample of simulated data, we have a file in `data/simulated_data.root`.\n",
    "\n",
    "**Exercise:** Load it into a pandas `DataFrame` called `mc_df`. Don't forget to add the `Jpsi_eta`, `mup_P` and `mum_P` columns!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_df = uproot.open('https://cern.ch/starterkit/data/advanced-python-2018/simulated_data.root',\n",
    "                    httpsource={'chunkbytes': 1024*1024, 'limitbytes': 33554432, 'parallel': 64}\n",
    "                    )['DecayTree'].pandas.df()\n",
    "mc_df.eval('Jpsi_eta = arctanh(Jpsi_PZ/Jpsi_P)', inplace=True)\n",
    "mc_df.eval('mup_P = sqrt(mum_PX**2 + mum_PY**2 + mum_PZ**2)', inplace=True)\n",
    "mc_df.eval('mum_P = sqrt(mum_PX**2 + mum_PY**2 + mum_PZ**2)', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION:** What can we to get a background sample?\n",
    "\n",
    "Sidebands, we know the peak is only present in $3.0~\\text{GeV} < M(J/\\psi) < 3.2~\\text{GeV}$. If we select events outside the region we know it's a pure background sample.\n",
    "\n",
    "**Exercise:** Make a new `DataFrame` called `bkg_df` containing only events outside $3.0~\\text{GeV} < M(J/\\psi) < 3.2~\\text{GeV}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bkg_df = data_df.query('~(3.0 < Jpsi_M < 3.2)')\n",
    "plot_mass(bkg_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION:** Why is there a step at 3 GeV on this plot?\n",
    "\n",
    "It's a binning effect, we've appled a cut at 3.0 GeV but the nearest bin is $[2.9975, 3.005]$ so it is only partially filled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot the variables in MC and background to see what they look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = 'Jpsi_PT'\n",
    "_, bins, _ = plt.hist(mc_df[var], bins=100, histtype='step', label='MC')\n",
    "_, bins, _ = plt.hist(bkg_df[var], bins=bins, histtype='step', label='Background')\n",
    "plt.xlabel(var)\n",
    "plt.xlim(bins[0], bins[-1])\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Those are hard to compare!!!\n",
    "# We should add the density keyword argument to normalise the distributions\n",
    "var = 'Jpsi_PT'\n",
    "_, bins, _ = plt.hist(mc_df[var], bins=100, histtype='step', label='MC', density=1)\n",
    "_, bins, _ = plt.hist(bkg_df[var], bins=bins, histtype='step', label='Background', density=1)\n",
    "plt.xlabel(var)\n",
    "plt.xlim(bins[0], bins[-1])\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Make a function which plots both variables with the signature `plot_comparision(var, mc_df, bkg_df)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_comparision(var, mc_df, bkg_df):\n",
    "    _, bins, _ = plt.hist(mc_df[var], bins=100, histtype='step', label='MC', density=1)\n",
    "    _, bins, _ = plt.hist(bkg_df[var], bins=bins, histtype='step', label='Background', density=1)\n",
    "    plt.xlabel(var)\n",
    "    plt.xlim(bins[0], bins[-1])\n",
    "    plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use this function to plot all of the variables available in the data using `data_df.columns`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in data_df.columns:\n",
    "    plt.figure()\n",
    "    plot_comparision(var, mc_df, bkg_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to note:\n",
    "\n",
    " - This doesn't work for the $J/\\psi$ mass variable: We can only rely on this method if the variable is independent of mass. Fortunately we often want to do this as if a variable is heavily dependent on mass it can shape our distributions and can make it hard to know what is signal and what is background.\n",
    " - Muon mass is a fixed value for all muons: In this sample we have assumed the PDG value of the muon mass to allow us to calculate the energy component using only the information from the tracking detectors. This is often more precise than using calorimeters to measure $P_E$.\n",
    " - We got a warning about `More than 20 figures have been opened.`: Opening plots uses memory so if you open too many at the same time your scripts can be become slow or even crash. In this case we can ignore it as we only produce 30 plots but be careful if you ever make thousands of plots.\n",
    " - Pseudorapidity (eta) only goes between about 1 and 6: This dataset is supposed to look like vaugely like LHCb data where the detector only covers that region."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Look at the variables above and try to get a clean $J/\\psi$ mass peak and use the significance function to see how well you do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aside:** We will want to use some of the variables and dataframes in the next lesson. In order to do this we will *store* them in this session and reload them in the next lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store bkg_df\n",
    "%store mc_df\n",
    "%store data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
