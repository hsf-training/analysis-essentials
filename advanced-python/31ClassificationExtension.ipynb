{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4: Extension on Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative implimentations\n",
    "\n",
    "Because `scikit-learn` defines a flexible standardised interface independent packages normally impliment the same interface to make it easy to switch and find the best one for you.\n",
    "\n",
    "`XGBoost` is one such example that is often ranked highly in machine learning competitions. Let's see how it performs in comparision to two alorithms from `scikit-learn`.\n",
    "\n",
    "**Exercise:** This could be an exercise if you have lots of time\n",
    "\n",
    "First load the data that we need from the previous lesson and import the modules again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:40:00.013914071Z",
     "start_time": "2023-11-09T18:39:58.305187065Z"
    }
   },
   "outputs": [],
   "source": [
    "%store -r training_data\n",
    "%store -r training_columns\n",
    "%store -r mc_df\n",
    "%store -r bkg_df\n",
    "%store -r data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:40:00.401140920Z",
     "start_time": "2023-11-09T18:40:00.001945534Z"
    }
   },
   "outputs": [],
   "source": [
    "import mplhep\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import auc, roc_curve\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:40:00.424165539Z",
     "start_time": "2023-11-09T18:40:00.375142416Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_comparision(var, mc_df, bkg_df):\n",
    "    # create histograms\n",
    "    hsig, bins = np.histogram(mc_df[var], bins=60, density=1)\n",
    "    hbkg, bins = np.histogram(bkg_df[var], bins=bins, density=1)\n",
    "\n",
    "    mplhep.histplot((hsig, bins), label='MC Signal', )\n",
    "    mplhep.histplot(hbkg, bins=bins, label='Data Bkg')\n",
    "    plt.xlabel(var)\n",
    "    plt.xlim(bins[0], bins[-1])\n",
    "    plt.legend(loc='best')\n",
    "\n",
    "\n",
    "def plot_significance(bdt, training_data, training_columns, label=None):\n",
    "    y_score = bdt.predict_proba(training_data[training_columns])[:, 1]\n",
    "    fpr, tpr, thresholds = roc_curve(training_data['catagory'], y_score)\n",
    "\n",
    "    n_sig = 1200\n",
    "    n_bkg = 23000\n",
    "    S = n_sig * tpr\n",
    "    B = n_bkg * fpr\n",
    "    metric = S / np.sqrt(S + B)\n",
    "\n",
    "    plt.plot(thresholds, metric, label=label)\n",
    "    plt.xlabel('BDT cut value')\n",
    "    plt.ylabel('$\\\\frac{S}{\\\\sqrt{S+B}}$')\n",
    "    plt.xlim(0, 1.0)\n",
    "\n",
    "    optimal_cut = thresholds[np.argmax(metric)]\n",
    "    plt.axvline(optimal_cut, color='black', linestyle='--')\n",
    "\n",
    "\n",
    "def plot_roc(bdt, training_data, training_columns, label=None):\n",
    "    y_score = bdt.predict_proba(training_data[training_columns])[:, 1]\n",
    "    fpr, tpr, thresholds = roc_curve(training_data['catagory'], y_score)\n",
    "    area = auc(fpr, tpr)\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], color='grey', linestyle='--')\n",
    "    if label:\n",
    "        plt.plot(fpr, tpr, label=f'{label} (area = {area:.2f})')\n",
    "    else:\n",
    "        plt.plot(fpr, tpr, label=f'ROC curve (area = {area:.2f})')\n",
    "    plt.xlim(0.0, 1.0)\n",
    "    plt.ylim(0.0, 1.0)\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.legend(loc='lower right')\n",
    "    # We can make the plot look nicer by forcing the grid to be square\n",
    "    plt.gca().set_aspect('equal', adjustable='box')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:40:22.730604251Z",
     "start_time": "2023-11-09T18:40:00.386255547Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train the Gradient Booster\n",
    "bdt_1 = GradientBoostingClassifier(n_estimators=20)  # less estimator is faster, for demonstration, but 100-300 is usually better\n",
    "bdt_1.fit(training_data[training_columns], training_data['catagory'])\n",
    "for df in [mc_df, bkg_df, data_df, training_data]:\n",
    "    df['BDT'] = bdt_1.predict_proba(df[training_columns])[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:40:32.407167754Z",
     "start_time": "2023-11-09T18:40:22.738816458Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train the Adaptive Booster\n",
    "bdt_2 = AdaBoostClassifier(n_estimators=20)  # less estimator is faster, for demonstration, but 100-300 is usually better\n",
    "bdt_2.fit(training_data[training_columns], training_data['catagory'])\n",
    "for df in [mc_df, bkg_df, data_df, training_data]:\n",
    "    df['BDT_2'] = bdt_2.predict_proba(df[training_columns])[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:40:33.896052239Z",
     "start_time": "2023-11-09T18:40:32.406995877Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train XGBoost Classifier\n",
    "xgboost_bdt = XGBClassifier(n_estimators=20)  # less estimator is faster, for demonstration, but 100-300 is usually better\n",
    "xgboost_bdt.fit(training_data[training_columns], training_data['catagory'])\n",
    "for df in [mc_df, bkg_df, data_df, training_data]:\n",
    "    df['XGBoost_BDT'] = xgboost_bdt.predict_proba(df[training_columns])[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:40:37.959441690Z",
     "start_time": "2023-11-09T18:40:33.890325623Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plot_comparision('BDT', mc_df, bkg_df)\n",
    "plt.figure()\n",
    "plot_comparision('BDT_2', mc_df, bkg_df)\n",
    "plt.figure()\n",
    "plot_comparision('XGBoost_BDT', mc_df, bkg_df)\n",
    "\n",
    "plt.figure()\n",
    "for bdt in [bdt_1, bdt_2, xgboost_bdt]:\n",
    "    plot_significance(bdt, training_data, training_columns)\n",
    "\n",
    "plt.figure()\n",
    "for bdt in [bdt_1, bdt_2, xgboost_bdt]:\n",
    "    plot_roc(bdt, training_data, training_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments:\n",
    "\n",
    "- While `predict_proba` is always defined to be between 0 and 1, the actualy value is generally not very meaningful. In this case we see that the range of the AdaBoost classifier is different. More generally, the output of a classifier is _not_ a probability but something that is probability-like: there exists a bijective transformation for the output to transform it to a probability for each classifier. This however is not known in general and can be hard to find. In short: the _ranking_ matters, not the absolute value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** What happens if you put mass into the BDT?\n",
    " - Avoid variables which correlated with mass\n",
    " - Variables with a little correlation is okay, such as momentum. (Assuming your resolution is good)\n",
    "\n",
    "**Question:** What happens if real and simulated data are different?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:40:38.867894240Z",
     "start_time": "2023-11-09T18:40:37.963705203Z"
    }
   },
   "outputs": [],
   "source": [
    "for df in [mc_df, bkg_df, data_df, training_data]:\n",
    "    df['IPmin'] = np.min([df['mum_PT'], df['mup_PT']], axis=0)\n",
    "\n",
    "for df in [mc_df, bkg_df, data_df, training_data]:\n",
    "    df['IPdiff'] = np.abs(df['mum_PT'] - df['mup_PT'])\n",
    "\n",
    "plt.figure()\n",
    "plot_comparision('mum_IP', mc_df, bkg_df)\n",
    "\n",
    "plt.figure()\n",
    "plot_comparision('mup_IP', mc_df, bkg_df)\n",
    "\n",
    "plt.figure()\n",
    "plot_comparision('IPmin', mc_df, bkg_df)\n",
    "\n",
    "plt.figure()\n",
    "plot_comparision('IPdiff', mc_df, bkg_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how much better the BDT performs when our new variable is included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:40:45.395771938Z",
     "start_time": "2023-11-09T18:40:38.878807747Z"
    }
   },
   "outputs": [],
   "source": [
    "# bdtclass = GradientBoostingClassifier()\n",
    "bdtclass = XGBClassifier  # we could also use this one\n",
    "bdt_1 = bdtclass(n_estimators=20)  # less estimator is faster, for demonstration, but 100-300 is usually better\n",
    "bdt_1.fit(training_data[training_columns], training_data['catagory'])\n",
    "for df in [mc_df, bkg_df, data_df, training_data]:\n",
    "    df['BDT'] = bdt_1.predict_proba(df[training_columns])[:, 1]\n",
    "\n",
    "bdt_2 = bdtclass(n_estimators=20)  # less estimator is faster, for demonstration, but 100-300 is usually better\n",
    "training_columns_2 = training_columns + ['IPmin']\n",
    "bdt_2.fit(training_data[training_columns_2], training_data['catagory'])\n",
    "for df in [mc_df, bkg_df, data_df, training_data]:\n",
    "    df['BDT_2'] = bdt_2.predict_proba(df[training_columns_2])[:, 1]\n",
    "\n",
    "bdt_3 = bdtclass(n_estimators=20)  # less estimator is faster, for demonstration, but 100-300 is usually better\n",
    "training_columns_3 = training_columns + ['IPdiff']\n",
    "bdt_3.fit(training_data[training_columns_3], training_data['catagory'])\n",
    "for df in [mc_df, bkg_df, data_df, training_data]:\n",
    "    df['BDT_3'] = bdt_3.predict_proba(df[training_columns_3])[:, 1]\n",
    "\n",
    "plt.figure()\n",
    "plot_comparision('BDT', mc_df, bkg_df)\n",
    "plt.figure()\n",
    "plot_comparision('BDT_2', mc_df, bkg_df)\n",
    "plt.figure()\n",
    "plot_comparision('BDT_3', mc_df, bkg_df)\n",
    "\n",
    "plt.figure()\n",
    "plot_significance(bdt_1, training_data, training_columns, label='Original')\n",
    "plot_significance(bdt_2, training_data, training_columns_2, label='IPmin')\n",
    "plot_significance(bdt_3, training_data, training_columns_3, label='IPdiff')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "plot_roc(bdt_1, training_data, training_columns, label='Original')\n",
    "plot_roc(bdt_2, training_data, training_columns_2, label='IPmin')\n",
    "plot_roc(bdt_3, training_data, training_columns_3, label='IPdiff')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $k$-folding\n",
    "\n",
    "This is a technique to avoid losing some parts of your data by splitting it into test and training set. It is crucial e.g. in the scenario where a part of your training data _needs predictions_ such as when a sideband is used as the background.\n",
    "\n",
    "![k-folding](https://miro.medium.com/max/601/1*PdwlCactbJf8F8C7sP-3gw.png)\n",
    "\n",
    "The red tiles are used as a training set in order to predict the blue ones. This is done k times, resulting in k prediction for the k tiles we split into. We end up having predictions for our whole dataset!\n",
    "\n",
    "The validation is a holdout part of the data that we can use to estimate our actual performance. When we optimize our algorithm, we may overfit to the given dataset and overestimate our performance. By evaluating our \"best\" algorithm on a dataset that has not yet been seen at all, we get an unbiased estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go search for `scikit learn k-folding`.\n",
    "\n",
    " - https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html\n",
    "\n",
    "Look at the example section:\n",
    "\n",
    "```python\n",
    ">>> from sklearn.model_selection import KFold\n",
    ">>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n",
    ">>> y = np.array([1, 2, 3, 4])\n",
    ">>> kf = KFold(n_splits=2)\n",
    ">>> kf.get_n_splits(X)\n",
    "2\n",
    ">>> print(kf)\n",
    "KFold(n_splits=2, random_state=None, shuffle=False)\n",
    ">>> for train_index, test_index in kf.split(X):\n",
    "...    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "...    X_train, X_test = X[train_index], X[test_index]\n",
    "...    y_train, y_test = y[train_index], y[test_index]\n",
    "TRAIN: [2 3] TEST: [0 1]\n",
    "TRAIN: [0 1] TEST: [2 3]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Implement the above training for your favoride classifier in K-folding with k = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turn this into a scipt using argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:40:45.395898908Z",
     "start_time": "2023-11-09T18:40:44.896492177Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
