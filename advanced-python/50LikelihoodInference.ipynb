{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 8: Likelihood inference\n",
    "\n",
    "*(note: in order to run this notebook, first run 20 creating the data and 30, adding a BDT)*\n",
    "\n",
    "\n",
    "In most analysis, fitting a model to data is a crucial, last step in the chain in order to extract information about the parameters of interest.\n",
    "\n",
    "**Model**: First, this involves building a model that depends on various parameters - some that are of immediate interest to us (Parameter Of Interest, POI) and some that help to describe the observables that we expect in the data but do not have any relevant physical meaning such as detectoreffects (Nuisance Parameter).\n",
    "\n",
    "**Loss**: With the model and the data, a likelihood (or sometimes also a $\\chi^2$ loss) is built. This is a function of the parameters and reflects the _likelihood of finding the data given the parameters_. The most likely parameter combination is retrieved when maximising the likelihood; also called the Maximum Likelihood (ML) estimator. Except for trivial cases, this has to be done numerically.\n",
    "\n",
    "While this procedure returns an estimate of the _best-fit_, it does not say anything about the _uncertainty_ of our measurement. Therefore, advanced statistical methods are needed that perform toy studies; these are fits to generated samples under different conditions such as fixing a parameter to a certain value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scope of this tutorial\n",
    "\n",
    "Here, we focus on the libraries that are available to perform likelihood fits. For unbinned fits in Python, the libraries that we will consider here are [zfit](https://github.com/zfit/zfit) (likelihood model fitting library) and [hepstats](https://github.com/scikit-hep/hepstats) (higher level statistical inference; can use `zfit` models), both which are relatively young and written in pure Python.\n",
    "\n",
    "An alternative to be mentioned is [RooFit](http://roofit.sourceforge.net/) and [RooStats](https://twiki.cern.ch/twiki/bin/view/RooStats/WebHome), an older, well-proven, reliable C++ framework that has Python bindings and acts as a standard in HEP for this kind of fits.\n",
    "\n",
    "In case your analysis involved pure binned and templated fits, [pyhf](https://github.com/scikit-hep/pyhf) provides a specialized library for this kind of fits.\n",
    "\n",
    "## Getting started\n",
    "\n",
    "In order to get started with `zfit`, it is recommendable to have a look at the [zfit-tutorials](https://zfit-tutorials.readthedocs.io/en/latest/index.html) or the [recorded](https://www.youtube.com/playlist?list=PLAuzjeTfC3tNeP6o2sbCr3Jsa6ATvEYR6) [tutorials](https://www.youtube.com/watch?v=YDW-XxrSbns), which also contains `hepstats` tutorials; more on the latter can be found [here](https://github.com/scikit-hep/hepstats/tree/master/notebooks).\n",
    "\n",
    "Before we continue, it is highly recommended to follow the [zfit introduction tutorial](https://zfit-tutorials.readthedocs.io/en/latest/tutorials/introduction/README.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:41:04.548448189Z",
     "start_time": "2023-11-09T18:41:03.508890856Z"
    }
   },
   "outputs": [],
   "source": [
    "%store -r bkg_df\n",
    "%store -r mc_df\n",
    "%store -r data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:41:04.555465994Z",
     "start_time": "2023-11-09T18:41:04.545771470Z"
    }
   },
   "outputs": [],
   "source": [
    "import hepstats\n",
    "import matplotlib.pyplot as plt\n",
    "import mplhep\n",
    "import numpy as np\n",
    "import zfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:41:04.607549796Z",
     "start_time": "2023-11-09T18:41:04.552563192Z"
    }
   },
   "outputs": [],
   "source": [
    "# apply cuts\n",
    "query = 'BDT > 0.4'\n",
    "data_df.query(query, inplace=True)\n",
    "mc_df.query(query, inplace=True)\n",
    "\n",
    "# reduce the datasize for this example to make the fit more interesting\n",
    "fraction = 0.1  # how much to take of the original data\n",
    "data_df = data_df.sample(frac=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: write a fit to the data using `zfit`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:41:04.643172087Z",
     "start_time": "2023-11-09T18:41:04.628270711Z"
    }
   },
   "outputs": [],
   "source": [
    "obs = zfit.Space('Jpsi_M', limits=(2.8, 3.5))  # defining the observable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:41:04.643396920Z",
     "start_time": "2023-11-09T18:41:04.636406001Z"
    }
   },
   "outputs": [],
   "source": [
    "# bkg = zfit.Data.from_pandas(bkg_df['Jpsi_M'], obs=obs)\n",
    "# OR\n",
    "# obs_bkg = zfit.Space('Jpsi_M', limits=(2.8, 3.0)) + zfit.Space('Jpsi_M', limits=(3.2, 3.5))\n",
    "# bkg_two = zfit.Data.from_pandas(data_df['Jpsi_M'], obs=obs_bkg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:41:04.866062850Z",
     "start_time": "2023-11-09T18:41:04.643452582Z"
    }
   },
   "outputs": [],
   "source": [
    "mc = zfit.Data.from_pandas(mc_df['Jpsi_M'], obs=obs)\n",
    "data = zfit.Data.from_pandas(data_df['Jpsi_M'], obs=obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Difference of the two spaces\n",
    "\n",
    "While the first space is defined over the whole space from 2.8 to 3.5, the second consists of two distinct regions. Therefore we can use the original space and zfit applies the cut, the same as we did before to the `bkg_df`.\n",
    "\n",
    "The difference comes when using the normalization in the PDF: we can either normalize it over the whole range or only over part of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:41:04.959244148Z",
     "start_time": "2023-11-09T18:41:04.910984590Z"
    }
   },
   "outputs": [],
   "source": [
    "lambd = zfit.Parameter('lambda', -0.1, -2, 2)\n",
    "bkg_yield = zfit.Parameter('bkg_yield', 5000, 0, 200000, step_size=1)\n",
    "\n",
    "mu = zfit.Parameter('mu', 3.1, 2.9, 3.3)\n",
    "sigma = zfit.Parameter('sigma', 0.1, 0, 0.5)\n",
    "sig_yield = zfit.Parameter('sig_yield', 200, 0, 10000, step_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:41:04.959518485Z",
     "start_time": "2023-11-09T18:41:04.959061502Z"
    }
   },
   "outputs": [],
   "source": [
    "bkg_pdf = zfit.pdf.Exponential(lambd, obs=obs)\n",
    "bkg_pdf.set_yield(bkg_yield)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:41:04.959692598Z",
     "start_time": "2023-11-09T18:41:04.959203182Z"
    }
   },
   "outputs": [],
   "source": [
    "sig_pdf = zfit.pdf.Gauss(obs=obs, mu=mu, sigma=sigma)\n",
    "sig_pdf.set_yield(sig_yield)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:41:04.965883067Z",
     "start_time": "2023-11-09T18:41:04.959353512Z"
    }
   },
   "outputs": [],
   "source": [
    "model = zfit.pdf.SumPDF([bkg_pdf, sig_pdf])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting\n",
    "\n",
    "Plots can simply be made with `matplotlib` and `mplhep`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:41:04.966167348Z",
     "start_time": "2023-11-09T18:41:04.959441956Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_fit(model, data, nbins=30, ax=None):\n",
    "    # The function will be reused.\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "\n",
    "    lower, upper = data.space.limit1d\n",
    "\n",
    "    # Creates and histogram of the data and plots it with mplhep.\n",
    "    counts, bin_edges = np.histogram(data.unstack_x(), bins=nbins)\n",
    "    mplhep.histplot(counts, bins=bin_edges, histtype=\"errorbar\", yerr=True,\n",
    "                    label=\"Data\", ax=ax, color=\"black\")\n",
    "\n",
    "    binwidth = np.diff(bin_edges)[0]\n",
    "    x = np.linspace(lower, upper, num=1000)  # or tf.linspace\n",
    "\n",
    "    # Line plots of the total pdf and the sub-pdfs.\n",
    "    y = model.ext_pdf(x) * binwidth\n",
    "    ax.plot(x, y, label=\"total\", color=\"royalblue\")\n",
    "    for m, l, c in zip(model.get_models(), [\"background\", \"signal\"], [\"forestgreen\", \"crimson\"]):\n",
    "        ym = m.ext_pdf(x) * binwidth\n",
    "        ax.plot(x, ym, label=l, color=c)\n",
    "\n",
    "    plt.xlabel('$J/\\\\psi$ mass [GeV]')\n",
    "    ax.set_title(data.data_range.obs[0])\n",
    "    ax.set_xlim(lower, upper)\n",
    "    ax.legend(fontsize=15)\n",
    "\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:41:07.019191120Z",
     "start_time": "2023-11-09T18:41:04.959591758Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_fit(model, data)  # before the fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss\n",
    "\n",
    "Since we have now the models and the datasets, we can e.g. pre-fit the signal PDF to the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:41:07.303510860Z",
     "start_time": "2023-11-09T18:41:07.018115082Z"
    }
   },
   "outputs": [],
   "source": [
    "sig_nll = zfit.loss.UnbinnedNLL(sig_pdf, mc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It warns us that we are using a non-extended loss. The extended loss also includes to fit the yield while the normal one does not. Since we want to fit the _shape_ here only, we use the non-extended one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:41:07.315026018Z",
     "start_time": "2023-11-09T18:41:07.282948708Z"
    }
   },
   "outputs": [],
   "source": [
    "minimizer = zfit.minimize.Minuit()\n",
    "# minimizer = zfit.minimize.NLoptLBFGSV1()  # can be changed but maybe not as powerful as iminuit\n",
    "# minimizer = zfit.minimize.ScipySLSQPV1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:41:08.077766311Z",
     "start_time": "2023-11-09T18:41:07.283066840Z"
    }
   },
   "outputs": [],
   "source": [
    "minimizer.minimize(sig_nll)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixing parameters\n",
    "\n",
    "Sometimes we want to fix parameters obtained from MC, such as tailes. Here we will fix the `sigma`, just for demonstration purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:41:08.081842858Z",
     "start_time": "2023-11-09T18:41:08.067454337Z"
    }
   },
   "outputs": [],
   "source": [
    "sigma.floating = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:41:08.677325245Z",
     "start_time": "2023-11-09T18:41:08.071212416Z"
    }
   },
   "outputs": [],
   "source": [
    "nll = zfit.loss.ExtendedUnbinnedNLL(model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:41:09.481473179Z",
     "start_time": "2023-11-09T18:41:08.679126453Z"
    }
   },
   "outputs": [],
   "source": [
    "result = minimizer.minimize(nll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:41:09.501631838Z",
     "start_time": "2023-11-09T18:41:09.483882912Z"
    }
   },
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:41:10.998164125Z",
     "start_time": "2023-11-09T18:41:09.499282781Z"
    }
   },
   "outputs": [],
   "source": [
    "result.hesse()  # calculate hessian error\n",
    "result.errors()  # profile, using minos like uncertainty\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:41:11.291271535Z",
     "start_time": "2023-11-09T18:41:10.991252129Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_fit(model, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: use `hepstats` to check the significance of our signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:41:11.471911896Z",
     "start_time": "2023-11-09T18:41:11.293166815Z"
    }
   },
   "outputs": [],
   "source": [
    "from hepstats.hypotests.calculators import AsymptoticCalculator\n",
    "from hepstats.hypotests.parameters import POI\n",
    "\n",
    "# the null hypothesis\n",
    "sig_yield_poi = POI(sig_yield, 0)\n",
    "calculator = AsymptoticCalculator(input=result, minimizer=minimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is another calculator in `hepstats` called `FrequentistCalculator` which constructs the test statistic distribution $f(q_{0} |H_{0})$ with pseudo-experiments (toys), but it takes more time.\n",
    "\n",
    "The `Discovery` class is a high-level class that takes as input a calculator and a `POI` instance representing the null hypothesis, it basically asks the calculator to compute the p-value and also computes the signifance as\n",
    "\n",
    "\\begin{equation}\n",
    "Z = \\Phi^{-1}(1 - p_0).\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:41:13.160970054Z",
     "start_time": "2023-11-09T18:41:11.474037243Z"
    }
   },
   "outputs": [],
   "source": [
    "from hepstats.hypotests import Discovery\n",
    "\n",
    "discovery = Discovery(calculator=calculator, poinull=sig_yield_poi)\n",
    "discovery.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** play around! First things first: repeat the fit. The difference we will see is statistical fluctuation from the resampling of the data; we take only a fraction at random.\n",
    "\n",
    "Change the fraction of data that we have, the BDT cut, the signal model...\n",
    "\n",
    "Attention: it is easy to have a significance of `inf` here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
