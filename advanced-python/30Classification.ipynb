{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3: Multivariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson we will use 'Multivariate Analysis' to improve the signal significance of our data sample. This involves training a Boosted Decision Tree (**BDT**) which can distinguish between signal-like and background-like events. The BDT takes a number of input variables and makes a prediction on whether the event is signal or background.\n",
    "\n",
    "* First we need to load the data which we stored in the previous lesson and import the modules we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:25:09.454693438Z",
     "start_time": "2023-11-09T18:25:07.925906320Z"
    }
   },
   "outputs": [],
   "source": [
    "%store -r bkg_df\n",
    "%store -r mc_df\n",
    "%store -r data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:25:09.455020356Z",
     "start_time": "2023-11-09T18:25:08.698996144Z"
    }
   },
   "outputs": [],
   "source": [
    "import mplhep\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import uproot\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import auc, roc_curve\n",
    "from sklearn.model_selection import KFold\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rectangular cuts are give good simple selections however you can often get better performance by using a more advanced technique. This generally works by taking adavantage of the corellation between variables. For example if we look in two dimensions, `Jpsi_eta` and `mum_P`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:25:11.748577377Z",
     "start_time": "2023-11-09T18:25:09.007997107Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(mc_df['mup_PT'], mc_df['mum_PT'], s=1, marker=',', label='Signal')\n",
    "plt.scatter(bkg_df['mup_PT'], bkg_df['mum_PT'], s=1, marker=',', label='Background')\n",
    "plt.xlabel('mup_PT')\n",
    "plt.ylabel('mum_PT')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there if we cut out 2 (or more) dimensional regions we can remove background more effectively. This is the idea behind decision trees.\n",
    "\n",
    "#### TODO Add a diagram of a decision tree for the above plot\n",
    "\n",
    "In machine learning there is a concept known as \"boosting\" which comes from the question:\n",
    "\n",
    "> Can a set of weak learners create a single strong learner?\n",
    "\n",
    "I.e. Can a set of bad decision trees be combined to make a single algorithm that has good performance? Luckily the answer is yes and this is known as a boosted ensemable of decision trees. A BDT.\n",
    "(This can also be done with other classifers but BDT's are popular for solving HEP classification problems.)\n",
    "\n",
    "This might sound complicated but there is a popular Python package called `scikit-learn` which makes it easy to do machine learning in Python.\n",
    "\n",
    "* First choose some variables which have good discriminating power for the classifier to use\n",
    "* For this exercise we will use `GradientBoosingClassifier` from `scikit-learn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:25:11.762307655Z",
     "start_time": "2023-11-09T18:25:11.747609552Z"
    }
   },
   "outputs": [],
   "source": [
    "training_columns = [\n",
    "    'Jpsi_PT',\n",
    "    'mup_PT', 'mup_eta', 'mup_ProbNNmu',\n",
    "    'mum_PT', 'mum_eta', 'mum_ProbNNmu',\n",
    "]\n",
    "# Store training_columns\n",
    "%store training_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:25:11.882528359Z",
     "start_time": "2023-11-09T18:25:11.752531431Z"
    }
   },
   "outputs": [],
   "source": [
    "# We then define the classifier we want to use\n",
    "# bdt = GradientBoostingClassifier()  # we could also use this one\n",
    "bdt = XGBClassifier(n_estimators=20)  # less estimator is faster, for demonstration, but 100-300 is usually better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to teach the BDT what signal and background look like we have to train it using some reference samples. One of the useful things about `scikit-learn` is that all classifiers are trained in the same way, by calling the `fit(X, y)` method. Where `X` is a 2D array of features and `y` is the labels (signal/background/other/...).\n",
    "\n",
    "* We need to set the background and signal categories for our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:25:11.882858520Z",
     "start_time": "2023-11-09T18:25:11.795008544Z"
    }
   },
   "outputs": [],
   "source": [
    "bkg_df = bkg_df.copy()\n",
    "bkg_df['catagory'] = 0  # Use 0 for background\n",
    "mc_df['catagory'] = 1  # Use 1 for signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:25:12.365576579Z",
     "start_time": "2023-11-09T18:25:11.795121407Z"
    }
   },
   "outputs": [],
   "source": [
    "# Now merge the data together\n",
    "training_data = pd.concat([bkg_df, mc_df], copy=True, ignore_index=True)\n",
    "# Store training_data for later\n",
    "%store training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:25:13.766805931Z",
     "start_time": "2023-11-09T18:25:12.375467218Z"
    }
   },
   "outputs": [],
   "source": [
    "# We can now fit the BDT\n",
    "bdt.fit(training_data[training_columns], training_data['catagory'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once a classifier has been trained we can apply it to a dataset to get a variable tells us how signal or background like each candidate is. There are a few different functions to do this, but in this case we will use `bdt.predict_proba`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:25:13.798097938Z",
     "start_time": "2023-11-09T18:25:13.764223021Z"
    }
   },
   "outputs": [],
   "source": [
    "bdt.predict_proba(data_df[training_columns].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has returned a $N_\\text{samples}$ by $N_\\text{classes}$ array. The first column is the 'probability' that the candiate is catagory 0 (background) and the second column is the probability that the candidate is catagory 1 (signal). These probabilities sum to 1 for each event are only valid under certain assumptions which typically don't apply in HEP.\n",
    "\n",
    "When you have only two classes we typically just use the signal probability and treat it as a continuous variable that is defined between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:25:13.866038246Z",
     "start_time": "2023-11-09T18:25:13.787561597Z"
    }
   },
   "outputs": [],
   "source": [
    "# We can now use slicing to select column 1 in the array from for all rows\n",
    "probabilities = bdt.predict_proba(data_df[training_columns])[:,1]\n",
    "probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CHALLENGE:** Add a new column to each of `mc_df`, `bkg_df`, `data_df` and `training_data` called BDT which contains the signal probability of the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:25:14.147837572Z",
     "start_time": "2023-11-09T18:25:13.865725416Z"
    }
   },
   "outputs": [],
   "source": [
    "mc_df['BDT'] = bdt.predict_proba(mc_df[training_columns])[:,1]\n",
    "bkg_df['BDT'] = bdt.predict_proba(bkg_df[training_columns])[:,1]\n",
    "data_df['BDT'] = bdt.predict_proba(data_df[training_columns])[:,1]\n",
    "training_data['BDT'] = bdt.predict_proba(training_data[training_columns])[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively we can avoid copying the same line of code by taking advantage of the fact that dataframes are mutable. This is a good idea as fewer lines of code normally means fewer chances for bugs. In the lines above we could easily mix up the different `DataFrame` objects which can cause subtle bugs. If it crashes it's okay, but if you accidenatlly use the wrong dataset somewhere and end up with mistakes in your final result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:25:14.352377276Z",
     "start_time": "2023-11-09T18:25:14.147685274Z"
    }
   },
   "outputs": [],
   "source": [
    "for df in [mc_df, bkg_df, data_df, training_data]:\n",
    "    df['BDT'] = bdt.predict_proba(df[training_columns])[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CHALLENGE:** Use your `plot_comparision` function from earlier to compare the BDT response on data and sidebands.\n",
    "\n",
    "Reload functions from the previous lesson here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:25:14.364834431Z",
     "start_time": "2023-11-09T18:25:14.351502685Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_comparision(var, mc_df, bkg_df):\n",
    "    # create histograms\n",
    "    hsig, bins = np.histogram(mc_df[var], bins=60, density=1)\n",
    "    hbkg, bins = np.histogram(bkg_df[var], bins=bins, density=1)\n",
    "\n",
    "    mplhep.histplot((hsig, bins), label='MC Signal', )\n",
    "    mplhep.histplot(hbkg, bins=bins, label='Data Bkg')\n",
    "    plt.xlabel(var)\n",
    "    plt.xlim(bins[0], bins[-1])\n",
    "    plt.legend(loc='best')\n",
    "\n",
    "\n",
    "def plot_mass(df, **kwargs):\n",
    "    h, bins = np.histogram(df['Jpsi_M'], bins=100, range=[2.75, 3.5])\n",
    "    mplhep.histplot(h, bins, yerr=True, **kwargs)  # feel free to adjust\n",
    "    # You can also use LaTeX in the axis label\n",
    "    plt.xlabel('$J/\\\\psi$ mass [GeV]')\n",
    "    plt.xlim(bins[0], bins[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:25:14.602892746Z",
     "start_time": "2023-11-09T18:25:14.359199993Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_comparision('BDT', mc_df, bkg_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now choose a cut value and compare it to the selection we used earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:25:16.410750494Z",
     "start_time": "2023-11-09T18:25:14.601766515Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_mass(data_df, label='No cuts', density=1)\n",
    "\n",
    "data_with_cuts_df = data_df.query('Jpsi_PT > 4')\n",
    "plot_mass(data_with_cuts_df, label='$J/\\\\psi$ p$_T$ only', density=1)\n",
    "\n",
    "data_with_cuts_df = data_df.query('(Jpsi_PT > 4) & ((mum_ProbNNmu > 0.9) & (mup_ProbNNmu > 0.9))')\n",
    "plot_mass(data_with_cuts_df, label='$J/\\\\psi$ p$_T$ and muon PID', density=1)\n",
    "\n",
    "data_with_cuts_df = data_df.query('BDT > 0.95')\n",
    "plot_mass(data_with_cuts_df, label='Using BDT only', density=1)\n",
    "\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CHALLENGE:** Try to find the best possibly cut on your BDT and compare you score to earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:25:17.390975490Z",
     "start_time": "2023-11-09T18:25:16.409914497Z"
    }
   },
   "outputs": [],
   "source": [
    "# That would be too nice to use in analysis\n",
    "from python_lesson import check_truth\n",
    "\n",
    "check_truth(data_df.query('BDT > 0.95'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating classifier performance\n",
    "\n",
    "So far we have used a magical function to evalueate the classifier performance. Unfortuanately, we can't use such a tool when using analysis on real data as that would be too easy! Instead, we have a few other tools we can use to see how good a classifier is.\n",
    "\n",
    "\n",
    "We have already used one tool: the distributions of the classifier output on signal and background training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:25:17.576076450Z",
     "start_time": "2023-11-09T18:25:17.380970864Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_comparision('BDT', mc_df, bkg_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this you can guess find a cut which removes some background while keeping almost all of the signal signal.\n",
    "\n",
    "Another tool we have the the Receiver Operating Characteristic curve, also known as the ROC curve. This shows the effiency of the classifier on signal (true positive rate, `tpr`) against the ineffieicny of removing background (false positive rate, `fpr`). Each point on this curve corropsonds to a cut value threshold.\n",
    "\n",
    "To do this we can reuse the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:25:17.674307301Z",
     "start_time": "2023-11-09T18:25:17.573465928Z"
    }
   },
   "outputs": [],
   "source": [
    "y_score = bdt.predict_proba(training_data[training_columns])[:,1]\n",
    "fpr, tpr, thresholds = roc_curve(training_data['catagory'], y_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make the plot look nicer by:\n",
    " - Adding axis labels\n",
    " - Forcing the grid to be square\n",
    " - Adding a line which corrosponds to randomly guessing\n",
    "\n",
    "If the ROC curve goes close to or below this gray line it means the classifier is really bad!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:25:17.825716688Z",
     "start_time": "2023-11-09T18:25:17.695016918Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot([0, 1], [0, 1], color='grey', linestyle='--', label='Randomly guess')\n",
    "plt.plot(fpr, tpr, label=f'ROC curve')\n",
    "plt.xlim(0.0, 1.0)\n",
    "plt.ylim(0.0, 1.0)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc='lower right')\n",
    "# We can make the plot look nicer by forcing the grid to be square\n",
    "plt.gca().set_aspect('equal', adjustable='box')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The closer to the top left corner this curve the better the classifier is, another way of looking at this is the area under this curve. Let's add this it to the legend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:25:17.883519883Z",
     "start_time": "2023-11-09T18:25:17.824214208Z"
    }
   },
   "outputs": [],
   "source": [
    "area = auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:25:17.996072891Z",
     "start_time": "2023-11-09T18:25:17.871252944Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot([0, 1], [0, 1], color='grey', linestyle='--')\n",
    "plt.plot(fpr, tpr, label=f'ROC curve (area = {area:.2f})')\n",
    "plt.xlim(0.0, 1.0)\n",
    "plt.ylim(0.0, 1.0)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc='lower right')\n",
    "# We can make the plot look nicer by forcing the grid to be square\n",
    "plt.gca().set_aspect('equal', adjustable='box')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another thing we can consider is the significance of the signal. Multiple different metrics can be used for this but the most popular generanl purpose metric is $\\frac{S}{\\sqrt{S+B}}$, where $S$ is the number of signal events and $B$ is the number of background events. It is important to be careful when defining $S$ and $B$ so that is is only includes the number of events under the signal peak. This is often done by taking a range such within $\\pm 2 \\sigma$ of the peak.\n",
    "\n",
    "For the toy dataset used here appropriate numbers are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:25:18.048225663Z",
     "start_time": "2023-11-09T18:25:17.994427329Z"
    }
   },
   "outputs": [],
   "source": [
    "n_sig = 1200\n",
    "n_bkg = 23000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use these numbers to plot the significance as a function of cut value. The number of signal events after the cut, $S$ is $N_\\text{sig} \\times \\text{true positive rate}$. The number of background events after the cut, $B$ is $N_\\text{bkg} \\times \\text{false positive rate}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:25:18.048735130Z",
     "start_time": "2023-11-09T18:25:18.035311051Z"
    }
   },
   "outputs": [],
   "source": [
    "S = n_sig*tpr\n",
    "B = n_bkg*fpr\n",
    "metric = S/np.sqrt(S+B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then plots this as a function of the cut value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:25:18.256753265Z",
     "start_time": "2023-11-09T18:25:18.035655070Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(thresholds, metric)\n",
    "plt.xlabel('BDT cut value')\n",
    "plt.ylabel('$\\\\frac{S}{\\\\sqrt{S+B}}$')\n",
    "plt.xlim(0, 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CHALLENGE:** Get the cut value corrosponding to the peak of this curve and plot the mass peak for that cut value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:25:18.257317434Z",
     "start_time": "2023-11-09T18:25:18.243137106Z"
    }
   },
   "outputs": [],
   "source": [
    "optimal_index = np.argmax(metric)\n",
    "optimal_metric = metric[optimal_index]\n",
    "optimal_cut = thresholds[optimal_index]\n",
    "print(f'The optimal cut value is {optimal_cut:.2f} with and S/sqrt(S+B) of {optimal_metric:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:25:18.635953003Z",
     "start_time": "2023-11-09T18:25:18.243349315Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_mass(data_df, label='No cuts', density=1)\n",
    "\n",
    "data_with_cuts_df = data_df.query(f'BDT > {optimal_cut}')\n",
    "plot_mass(data_with_cuts_df, label='Using BDT only', density=1)\n",
    "\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:25:18.742133399Z",
     "start_time": "2023-11-09T18:25:18.551392406Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_mass(data_with_cuts_df, label='Using BDT only')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting it all together\n",
    "\n",
    "Let's define some helpful functions that we use later based on what we did above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:25:18.813621527Z",
     "start_time": "2023-11-09T18:25:18.743820685Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_roc(bdt, training_data, training_columns, label=None):\n",
    "    y_score = bdt.predict_proba(training_data[training_columns])[:,1]\n",
    "    fpr, tpr, thresholds = roc_curve(training_data['catagory'], y_score)\n",
    "    area = auc(fpr, tpr)\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], color='grey', linestyle='--')\n",
    "    if label:\n",
    "        plt.plot(fpr, tpr, label=f'{label} (area = {area:.2f})')\n",
    "    else:\n",
    "        plt.plot(fpr, tpr, label=f'ROC curve (area = {area:.2f})')\n",
    "    plt.xlim(0.0, 1.0)\n",
    "    plt.ylim(0.0, 1.0)\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.legend(loc='lower right')\n",
    "    # We can make the plot look nicer by forcing the grid to be square\n",
    "    plt.gca().set_aspect('equal', adjustable='box')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:25:18.813858248Z",
     "start_time": "2023-11-09T18:25:18.791163354Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_significance(bdt, training_data, training_columns, label=None):\n",
    "    y_score = bdt.predict_proba(training_data[training_columns])[:,1]\n",
    "    fpr, tpr, thresholds = roc_curve(training_data['catagory'], y_score)\n",
    "\n",
    "    n_sig = 1200\n",
    "    n_bkg = 23000\n",
    "    S = n_sig*tpr\n",
    "    B = n_bkg*fpr\n",
    "    metric = S/np.sqrt(S+B)\n",
    "\n",
    "    plt.plot(thresholds, metric, label=label)\n",
    "    plt.xlabel('BDT cut value')\n",
    "    plt.ylabel('$\\\\frac{S}{\\\\sqrt{S+B}}$')\n",
    "    plt.xlim(0, 1.0)\n",
    "\n",
    "    optimal_cut = thresholds[np.argmax(metric)]\n",
    "    plt.axvline(optimal_cut, color='black', linestyle='--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:30:20.710903847Z",
     "start_time": "2023-11-09T18:25:18.791369479Z"
    }
   },
   "outputs": [],
   "source": [
    "with uproot.open('https://cern.ch/starterkit/data/advanced-python-2018/real_data.root') as datafile:\n",
    "    data_df = datafile['DecayTree'].arrays(library='pd')\n",
    "with uproot.open('https://cern.ch/starterkit/data/advanced-python-2018/simulated_data.root') as mcfile:\n",
    "    mc_df = mcfile['DecayTree'].arrays(library='pd')\n",
    "\n",
    "print(\"Data succesfully loaded\")\n",
    "bkg_df = data_df.query('~(3.0 < Jpsi_M < 3.2)').copy()\n",
    "\n",
    "for df in [mc_df, data_df, bkg_df]:\n",
    "    df.eval('Jpsi_eta = arctanh(Jpsi_PZ/Jpsi_P)', inplace=True)\n",
    "    df.eval('mup_P = sqrt(mum_PX**2 + mum_PY**2 + mum_PZ**2)', inplace=True)\n",
    "    df.eval('mum_P = sqrt(mum_PX**2 + mum_PY**2 + mum_PZ**2)', inplace=True)\n",
    "\n",
    "training_columns = [\n",
    "    'Jpsi_PT',\n",
    "    'mup_PT', 'mup_eta', 'mup_ProbNNmu', 'mup_IP',\n",
    "    'mum_PT', 'mum_eta', 'mum_ProbNNmu', 'mum_IP',\n",
    "]\n",
    "\n",
    "bkg_df['catagory'] = 0  # Use 0 for background\n",
    "mc_df['catagory'] = 1  # Use 1 for signal\n",
    "training_data = pd.concat([bkg_df, mc_df], copy=True, ignore_index=True)\n",
    "\n",
    "bdt = GradientBoostingClassifier()\n",
    "bdt.fit(training_data[training_columns], training_data['catagory'])\n",
    "\n",
    "mc_df['BDT'] = bdt.predict_proba(mc_df[training_columns])[:,1]\n",
    "bkg_df['BDT'] = bdt.predict_proba(bkg_df[training_columns])[:,1]\n",
    "data_df['BDT'] = bdt.predict_proba(data_df[training_columns])[:,1]\n",
    "training_data['BDT'] = bdt.predict_proba(training_data[training_columns])[:,1]\n",
    "\n",
    "plt.figure()\n",
    "plot_comparision('BDT', mc_df, bkg_df)\n",
    "\n",
    "plt.figure()\n",
    "plot_significance(bdt, training_data, training_columns)\n",
    "\n",
    "plt.figure()\n",
    "plot_roc(bdt, training_data, training_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:30:21.685114217Z",
     "start_time": "2023-11-09T18:30:20.708314393Z"
    }
   },
   "outputs": [],
   "source": [
    "%store bkg_df\n",
    "%store mc_df\n",
    "%store data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:30:21.727449098Z",
     "start_time": "2023-11-09T18:30:21.679809042Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
