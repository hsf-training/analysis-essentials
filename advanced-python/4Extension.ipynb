{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4: Extension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative implimentations\n",
    "\n",
    "Because `scikit-learn` defines a flexible standardised interface independent packages normally impliment the same interface to make it easy to switch and find the best one for you.\n",
    "\n",
    "`XGBoost` is one such example that is often ranked highly in machine learning competitions. Let's see how it performs in comparision to two alorithms from `scikit-learn`.\n",
    "\n",
    "**Exercise:** This could be an exercise if you have lots of time\n",
    "\n",
    "First load the data that we need from the previous lesson and import the modules again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r training_data\n",
    "%store -r training_columns\n",
    "%store -r mc_df\n",
    "%store -r bkg_df\n",
    "%store -r data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import uproot\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_comparision(var, mc_df, bkg_df):\n",
    "    _, bins, _ = plt.hist(mc_df[var], bins=100, histtype='step', label='MC', density=1)\n",
    "    _, bins, _ = plt.hist(bkg_df[var], bins=bins, histtype='step', label='Background', density=1)\n",
    "    plt.xlabel(var)\n",
    "    plt.xlim(bins[0], bins[-1])\n",
    "    plt.legend(loc='best')\n",
    "    \n",
    "def plot_significance(bdt, training_data, training_columns, label=None):\n",
    "    y_score = bdt.predict_proba(training_data[training_columns])[:,1]\n",
    "    fpr, tpr, thresholds = roc_curve(training_data['catagory'], y_score)\n",
    "\n",
    "    n_sig = 1200\n",
    "    n_bkg = 23000\n",
    "    S = n_sig*tpr\n",
    "    B = n_bkg*fpr\n",
    "    metric = S/np.sqrt(S+B)\n",
    "\n",
    "    plt.plot(thresholds, metric, label=label)\n",
    "    plt.xlabel('BDT cut value')\n",
    "    plt.ylabel('$\\\\frac{S}{\\\\sqrt{S+B}}$')\n",
    "    plt.xlim(0, 1.0)\n",
    "\n",
    "    optimal_cut = thresholds[np.argmax(metric)]\n",
    "    plt.axvline(optimal_cut, color='black', linestyle='--')\n",
    "    \n",
    "def plot_roc(bdt, training_data, training_columns, label=None):\n",
    "    y_score = bdt.predict_proba(training_data[training_columns])[:,1]\n",
    "    fpr, tpr, thresholds = roc_curve(training_data['catagory'], y_score)\n",
    "    area = auc(fpr, tpr)\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], color='grey', linestyle='--')\n",
    "    if label:\n",
    "        plt.plot(fpr, tpr, label=f'{label} (area = {area:.2f})')\n",
    "    else:\n",
    "        plt.plot(fpr, tpr, label=f'ROC curve (area = {area:.2f})')\n",
    "    plt.xlim(0.0, 1.0)\n",
    "    plt.ylim(0.0, 1.0)\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.legend(loc='lower right')\n",
    "    # We can make the plot look nicer by forcing the grid to be square\n",
    "    plt.gca().set_aspect('equal', adjustable='box')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bdt_1 = GradientBoostingClassifier()\n",
    "bdt_1.fit(training_data[training_columns], training_data['catagory'])\n",
    "for df in [mc_df, bkg_df, data_df, training_data]:\n",
    "    df['BDT'] = bdt_1.predict_proba(df[training_columns])[:,1]\n",
    "\n",
    "bdt_2 = AdaBoostClassifier()\n",
    "bdt_2.fit(training_data[training_columns], training_data['catagory'])\n",
    "for df in [mc_df, bkg_df, data_df, training_data]:\n",
    "    df['BDT_2'] = bdt_2.predict_proba(df[training_columns])[:,1]\n",
    "\n",
    "xgboost_bdt = XGBClassifier()\n",
    "xgboost_bdt.fit(training_data[training_columns], training_data['catagory'])\n",
    "for df in [mc_df, bkg_df, data_df, training_data]:\n",
    "    df['XGBoost_BDT'] = xgboost_bdt.predict_proba(df[training_columns])[:,1]\n",
    "\n",
    "plt.figure()\n",
    "plot_comparision('BDT', mc_df, bkg_df)\n",
    "plt.figure()\n",
    "plot_comparision('BDT_2', mc_df, bkg_df)\n",
    "plt.figure()\n",
    "plot_comparision('XGBoost_BDT', mc_df, bkg_df)\n",
    "\n",
    "plt.figure()\n",
    "for bdt in [bdt_1, bdt_2, xgboost_bdt]:\n",
    "    plot_significance(bdt, training_data, training_columns)\n",
    "\n",
    "plt.figure()\n",
    "for bdt in [bdt_1, bdt_2, xgboost_bdt]:\n",
    "    plot_roc(bdt, training_data, training_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments:\n",
    "\n",
    "- While `predict_proba` is always defined to be between 0 and 1, the actualy value is generally not very meaningful. In this case we see that the range of the AdaBoost classifier is different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** What happens if you put mass into the BDT?\n",
    " - Avoid variables which correlated with mass\n",
    " - Variables with a little correlation is okay, such as momentum. (Assuming your resolution is good)\n",
    "\n",
    "**Question:** What happens if real and simulated data are different?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [mc_df, bkg_df, data_df, training_data]:\n",
    "    df['IPmin'] = np.min([df['mum_PT'], df['mup_PT']], axis=0)\n",
    "\n",
    "for df in [mc_df, bkg_df, data_df, training_data]:\n",
    "    df['IPdiff'] = np.abs(df['mum_PT'] - df['mup_PT'])\n",
    "\n",
    "plt.figure()\n",
    "plot_comparision('mum_IP', mc_df, bkg_df)\n",
    "\n",
    "plt.figure()\n",
    "plot_comparision('mup_IP', mc_df, bkg_df)\n",
    "\n",
    "plt.figure()\n",
    "plot_comparision('IPmin', mc_df, bkg_df)\n",
    "\n",
    "plt.figure()\n",
    "plot_comparision('IPdiff', mc_df, bkg_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how much better the BDT performs when our new variable is included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bdt_1 = GradientBoostingClassifier()\n",
    "bdt_1.fit(training_data[training_columns], training_data['catagory'])\n",
    "for df in [mc_df, bkg_df, data_df, training_data]:\n",
    "    df['BDT'] = bdt_1.predict_proba(df[training_columns])[:,1]\n",
    "\n",
    "bdt_2 = GradientBoostingClassifier()\n",
    "training_columns_2 = training_columns + ['IPmin']\n",
    "bdt_2.fit(training_data[training_columns_2], training_data['catagory'])\n",
    "for df in [mc_df, bkg_df, data_df, training_data]:\n",
    "    df['BDT_2'] = bdt_2.predict_proba(df[training_columns_2])[:,1]\n",
    "\n",
    "\n",
    "bdt_3 = GradientBoostingClassifier()\n",
    "training_columns_3 = training_columns + ['IPdiff']\n",
    "bdt_3.fit(training_data[training_columns_3], training_data['catagory'])\n",
    "for df in [mc_df, bkg_df, data_df, training_data]:\n",
    "    df['BDT_3'] = bdt_3.predict_proba(df[training_columns_3])[:,1]\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plot_comparision('BDT', mc_df, bkg_df)\n",
    "plt.figure()\n",
    "plot_comparision('BDT_2', mc_df, bkg_df)\n",
    "plt.figure()\n",
    "plot_comparision('BDT_3', mc_df, bkg_df)\n",
    "\n",
    "plt.figure()\n",
    "plot_significance(bdt_1, training_data, training_columns, label='Original')\n",
    "plot_significance(bdt_2, training_data, training_columns_2, label='IPmin')\n",
    "plot_significance(bdt_3, training_data, training_columns_3, label='IPdiff')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "plot_roc(bdt_1, training_data, training_columns, label='Original')\n",
    "plot_roc(bdt_2, training_data, training_columns_2, label='IPmin')\n",
    "plot_roc(bdt_3, training_data, training_columns_3, label='IPdiff')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $k$-folding\n",
    "\n",
    "Let's go search for `scikit learn k-folding`.\n",
    "\n",
    " - https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html\n",
    "\n",
    "Look at the example section:\n",
    "\n",
    "```python\n",
    ">>> from sklearn.model_selection import KFold\n",
    ">>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n",
    ">>> y = np.array([1, 2, 3, 4])\n",
    ">>> kf = KFold(n_splits=2)\n",
    ">>> kf.get_n_splits(X)\n",
    "2\n",
    ">>> print(kf)  \n",
    "KFold(n_splits=2, random_state=None, shuffle=False)\n",
    ">>> for train_index, test_index in kf.split(X):\n",
    "...    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "...    X_train, X_test = X[train_index], X[test_index]\n",
    "...    y_train, y_test = y[train_index], y[test_index]\n",
    "TRAIN: [2 3] TEST: [0 1]\n",
    "TRAIN: [0 1] TEST: [2 3]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turn this into a scipt using argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
