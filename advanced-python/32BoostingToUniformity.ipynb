{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5: Boosting to Uniformity\n",
    "\n",
    "In physics applications frequently we need to achieve uniformity of predictions along some features.\n",
    "For instance, when testing for the existence of a new particle, we need a classifier to be uniform in background along the mass (otherwise one can get false discovery due to so-called peaking background).\n",
    "\n",
    "This notebook contains some comparison of classifiers. The target is to obtain flat effiency in __signal__ (without significally loosing quality of classification) in Dalitz features.\n",
    "\n",
    "The classifiers compared are\n",
    "\n",
    "* plain __GradientBoosting__\n",
    "* __uBoost__\n",
    "* gradient boosting with knn-Ada loss (__UGB+knnAda__)\n",
    "* gradient boosting with FlatnessLoss (__UGB+FlatnessLoss__)\n",
    "\n",
    "We use dataset from paper about `uBoost` for demonstration purposes.\n",
    "We have a plenty of data here, so results are quite stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:25:39.380065511Z",
     "start_time": "2023-11-09T18:25:38.723060607Z"
    }
   },
   "outputs": [],
   "source": [
    "import uproot\n",
    "from hep_ml import gradientboosting as ugb\n",
    "from hep_ml import losses, uboost\n",
    "# this wrapper makes it possible to train on subset of features\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:26:25.983039940Z",
     "start_time": "2023-11-09T18:25:39.055897465Z"
    }
   },
   "outputs": [],
   "source": [
    "used_columns = [\"Y1\", \"Y2\", \"Y3\", \"M2AB\", \"M2AC\"]\n",
    "with uproot.open('https://cern.ch/starterkit/data/advanced-python-2019/dalitzdata.root') as datafile:\n",
    "    data = datafile['tree'].arrays(library='pd')\n",
    "labels = data['labels']\n",
    "data = data.drop('labels', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributions in the Dalitz features for signal and background\n",
    "As we can see, the background is distributed mostly in the corners of Dalitz plot, <br />\n",
    "and for traditional classifiers this results in poor effieciency of signal in the corners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:26:26.722476388Z",
     "start_time": "2023-11-09T18:26:26.022942250Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_distribution(data_frame, var_name1='M2AB', var_name2='M2AC', bins=40):\n",
    "    \"\"\"The function to plot 2D distribution histograms\"\"\"\n",
    "    plt.hist2d(data_frame[var_name1], data_frame[var_name2], bins = 40, cmap=plt.cm.Blues)\n",
    "    plt.xlabel(var_name1)\n",
    "    plt.ylabel(var_name2)\n",
    "    plt.colorbar()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1), plt.title(\"signal\"),       plot_distribution(data[labels==1])\n",
    "plt.subplot(1, 2, 2), plt.title(\"background\"),   plot_distribution(data[labels==0]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation of train/test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:26:26.738984299Z",
     "start_time": "2023-11-09T18:26:26.722774930Z"
    }
   },
   "outputs": [],
   "source": [
    "trainX, testX, trainY, testY = train_test_split(data, labels, random_state=42, test_size=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up classifiers, training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:26:26.847926700Z",
     "start_time": "2023-11-09T18:26:26.737604646Z"
    }
   },
   "outputs": [],
   "source": [
    "uniform_features  = [\"M2AB\", \"M2AC\"]\n",
    "train_features = [\"Y1\", \"Y2\", \"Y3\"]\n",
    "n_estimators = 150\n",
    "base_estimator = DecisionTreeClassifier(max_depth=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__uBoost__ training takes much time, so we reduce number of efficiency_steps, use prediction smoothing and run uBoost in threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:30:50.832483520Z",
     "start_time": "2023-11-09T18:26:26.744692640Z"
    }
   },
   "outputs": [],
   "source": [
    "classifiers = {}\n",
    "\n",
    "classifiers['AdaBoost'] = GradientBoostingClassifier(max_depth=4, n_estimators=n_estimators, learning_rate=0.1)\n",
    "\n",
    "\n",
    "knnloss = ugb.KnnAdaLossFunction(uniform_features, knn=10, uniform_label=1)\n",
    "classifiers['uGB+knnAda'] = ugb.UGradientBoostingClassifier(loss=knnloss, max_depth=4, n_estimators=n_estimators,\n",
    "                                        learning_rate=0.4, train_features=train_features)\n",
    "\n",
    "classifiers['uBoost'] = uboost.uBoostClassifier(uniform_features=uniform_features, uniform_label=1,\n",
    "                                     base_estimator=base_estimator,\n",
    "                                     n_estimators=n_estimators, train_features=train_features,\n",
    "                                     efficiency_steps=12, n_threads=4)\n",
    "\n",
    "flatnessloss = ugb.KnnFlatnessLossFunction(uniform_features, fl_coefficient=3., power=1.3, uniform_label=1)\n",
    "classifiers['uGB+FL'] = ugb.UGradientBoostingClassifier(loss=flatnessloss, max_depth=4,\n",
    "                                       n_estimators=n_estimators,\n",
    "                                       learning_rate=0.1, train_features=train_features)\n",
    "\n",
    "for clf in classifiers.values():\n",
    "    if clf==classifiers['AdaBoost']:\n",
    "        clf.fit(trainX[train_features], trainY)\n",
    "    else:\n",
    "        clf.fit(trainX[train_features + uniform_features], trainY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's look at the results of training\n",
    "\n",
    "dependence of classification quality on the number of trees built (ROC AUC - an area under the ROC curve, the more the better)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:30:57.270917786Z",
     "start_time": "2023-11-09T18:30:50.874988212Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "for clf in classifiers.values():\n",
    "    if clf==classifiers['AdaBoost']:\n",
    "        output = clf.predict_proba(trainX[train_features])\n",
    "    else:\n",
    "        output = clf.predict_proba(trainX[train_features + uniform_features])\n",
    "    print(f'Area under curve: {roc_auc_score(trainY, output[:,1])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC curves after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:31:04.075394547Z",
     "start_time": "2023-11-09T18:30:57.271060612Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "\n",
    "def plot_roc(bdt, testY, testX, training_columns, label=None):\n",
    "    y_score = bdt.predict_proba(testX[training_columns])[:,1]\n",
    "    fpr, tpr, thresholds = roc_curve(testY, y_score)\n",
    "    area = roc_auc_score(testY, y_score)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], color='grey', linestyle='--')\n",
    "    if label:\n",
    "        plt.plot(fpr, tpr, label=f'{label} (area = {area:.2f})')\n",
    "    else:\n",
    "        plt.plot(fpr, tpr, label=f'ROC curve (area = {area:.2f})')\n",
    "    plt.xlim(0.0, 1.0)\n",
    "    plt.ylim(0.0, 1.0)\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.legend(loc='lower right')\n",
    "    # We can make the plot look nicer by forcing the grid to be square\n",
    "    plt.gca().set_aspect('equal', adjustable='box')\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "plot_roc(classifiers['AdaBoost'], testY, testX, train_features, 'AdaBoost')\n",
    "plot_roc(classifiers['uGB+knnAda'], testY, testX, train_features+uniform_features, 'uGB+knnAda')\n",
    "plot_roc(classifiers['uBoost'], testY, testX, train_features+uniform_features, 'uBoost')\n",
    "plot_roc(classifiers['uGB+FL'], testY, testX, train_features+uniform_features, 'uGB+FL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:31:04.079427234Z",
     "start_time": "2023-11-09T18:31:04.071407036Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "nbsphinx": {
   "execute": "never"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
