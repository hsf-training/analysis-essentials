{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9: sPlot\n",
    "\n",
    "This notebook explains __sPlot__ and how to use `zfit` and `hepstats` to compute the sWeights with `compute_sweights`. Alternatively, if the probabilities are already available, `hep_ml.splot` can be used.\n",
    "__sPlot__ is a way to reconstruct features of mixture components based on known properties of distributions. This method is frequently used in High Energy Physics.\n",
    "\n",
    "If you prefer explanations without code, find them [here](http://arogozhnikov.github.io/2015/10/07/splot.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:41:33.690188079Z",
     "start_time": "2023-11-09T18:41:33.174962617Z"
    }
   },
   "outputs": [],
   "source": [
    "import mplhep\n",
    "import numpy as np\n",
    "import zfit\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:41:33.690535258Z",
     "start_time": "2023-11-09T18:41:33.175105562Z"
    }
   },
   "outputs": [],
   "source": [
    "size = 10000\n",
    "sig_data = np.random.normal(-1, 1, size=size)\n",
    "bck_data = np.random.normal(1, 1, size=size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple sPlot example\n",
    "\n",
    "First we start with a simple (and not very useful in practice) example:\n",
    "\n",
    "- Assume we have two types of particles (say, electrons and positrons).\n",
    "- Distribution of some characteristic is different for them (let this be $p_x$ momentum projection)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:41:35.114704154Z",
     "start_time": "2023-11-09T18:41:33.175205312Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.hist(sig_data, color='b', alpha=0.5, bins=30, label='electron')\n",
    "plt.hist(bck_data, color='r', alpha=0.5, bins=30, label='positron')\n",
    "plt.xlim(-5, 5), plt.xlabel('$p_x$')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observed distributions\n",
    "\n",
    "Picture above shows how this distibution should look like,\n",
    "but due to inaccuracies during classification we will observe a different picture.\n",
    "\n",
    "Let's assume that with a probability of 80% particle is classified correctly (and we are not using $p_x$ during classification).\n",
    "\n",
    "And when we look at distribution of px for particles which were classified as electrons or positrons, we see that they were distorted.\n",
    "We lost the original shapes of distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:41:35.169658981Z",
     "start_time": "2023-11-09T18:41:35.114998403Z"
    }
   },
   "outputs": [],
   "source": [
    "n_sig1, n_bck1 = 8000, 2000\n",
    "n_sig2, n_bck2 = 2000, 8000\n",
    "first_bin = np.concatenate([sig_data[:n_sig1], bck_data[:n_bck1]])\n",
    "second_bin = np.concatenate([sig_data[n_sig1:], bck_data[n_bck1:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:41:35.640708376Z",
     "start_time": "2023-11-09T18:41:35.159083078Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=[15, 6])\n",
    "plt.subplot(121)\n",
    "plt.bar([0, 2], [n_sig1, n_sig2], width=1, color='b', alpha=0.5)\n",
    "plt.bar([0, 2], [n_bck1, n_bck2], width=1, bottom=[n_sig1, n_sig2], color='r', alpha=0.5)\n",
    "plt.xlim(-0.5, 3.5)\n",
    "\n",
    "plt.axis('off')\n",
    "plt.xticks([0.5, 2.5], ['as electrons', 'as positrons'])\n",
    "plt.text(0.5, -300, 'as electron', horizontalalignment='center', verticalalignment='top', fontsize=20)\n",
    "plt.text(2.5, -300, 'as positron', horizontalalignment='center', verticalalignment='top', fontsize=20)\n",
    "plt.title('Proportion of events being classified as')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.hist(first_bin, alpha=0.5, bins=30, label='as electrons', color=(0.22, 0., 0.66))\n",
    "plt.hist(second_bin, alpha=0.5, bins=30, label='as positrons', color=(0.66, 0., 0.22))\n",
    "plt.legend()\n",
    "plt.title('Distributions')\n",
    "plt.xlim(-5, 5), plt.xlabel('$p_x$')\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying sWeights\n",
    "\n",
    "We can think of it in the following way: there are 2 bins. In first 80% are electrons, 20% are signal. And visa versa in second bin.\n",
    "\n",
    "To reconstruct initial distribution, we can plot histogram, where each event from first bin has weight 0.8,\n",
    "and each event from second bin has weight -0.2. This numbers are called sWeights.\n",
    "\n",
    "So, if we had 8000 $e^{-}$ + 2000 $e^{+}$ in first bin and 8000 $e^{+}$ + 2000 $e^{-}$ ($ e^-, e^+$ are electron and positron). After summing with introduced sWeights:\n",
    "\n",
    "$$\n",
    "\\big[ 8000 e^{-} + 2000 e^{+} \\big] \\times 0.8 + \\big[ 2000 e^{-} + 8000 e^{+} \\big] \\times (- 0.2) =\n",
    "6800 e^{-}\n",
    "$$\n",
    "\n",
    "Positrons with positive and negative weights compensated each other, and we will get pure electrons.\n",
    "\n",
    "At this moment we ignore normalization of sWeights (because it doesn't play role when we want to reconstruct shape).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:41:35.761702659Z",
     "start_time": "2023-11-09T18:41:35.641363489Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_with_weights(datas, weights, **kargs):\n",
    "    assert len(datas) == len(weights)\n",
    "    data = np.concatenate(datas)\n",
    "    weight = np.concatenate([np.ones(len(d)) * w for d, w in zip(datas, weights) ])\n",
    "    plt.hist(data, weights=weight, alpha=0.5, bins=30, **kargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:41:35.974742303Z",
     "start_time": "2023-11-09T18:41:35.682988823Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_with_weights([first_bin, second_bin], [n_bck2, -n_bck1], density=True, label='reconstructed electron')\n",
    "plot_with_weights([first_bin, second_bin], [-n_sig2, n_sig1], density=True, color='r', label='reconstructed positron')\n",
    "plt.xlabel('px')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare\n",
    "\n",
    "let's compare reconstructed distribution for electrons with an original one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:41:36.278752546Z",
     "start_time": "2023-11-09T18:41:35.973225726Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_with_weights([first_bin, second_bin], [n_bck2, -n_bck1], density=True, label='reconstructed electons', edgecolor='none')\n",
    "plot_with_weights([sig_data], [1], density=True, label='original electons', edgecolor='none')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More complex case\n",
    "\n",
    "In the case when we have only two 'bins' is simple and straightforward. But when there are more than two bins, the solution is not unique. There are many appropriate combinations of sWeights, which one to choose?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:41:36.370668105Z",
     "start_time": "2023-11-09T18:41:36.281084677Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.bar([0, 2, 4], [3, 2, 1], width=1, color='b', alpha=0.5)\n",
    "plt.bar([0, 2, 4], [1, 2, 3], width=1, bottom=[3, 2, 1], color='r', alpha=0.5)\n",
    "\n",
    "plt.xlim(-1, 6)\n",
    "plt.ylim(-0.5, 5)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.text(0.5, -0.5, 'Bin 1', horizontalalignment='center', verticalalignment='top', fontsize=20)\n",
    "plt.text(2.5, -0.5, 'Bin 2', horizontalalignment='center', verticalalignment='top', fontsize=20)\n",
    "plt.text(4.5, -0.5, 'Bin 3', horizontalalignment='center', verticalalignment='top', fontsize=20)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things in practice are however even more complex. We have not bins, but continuos distribution (which can be treated as many bins).\n",
    "\n",
    "Typically this is a distribution over mass. By fitting mass we are able to split mixture into two parts: signal channel and everything else."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Splot\n",
    "\n",
    "This is now an demonstration of the **sPlot** algorithm, described in [Pivk:2004ty](https://arxiv.org/pdf/physics/0402083.pdf).\n",
    "\n",
    "If a data sample is populated by different sources of events, like signal and background, **sPlot** is able to unfold the contributions of the different sources for a given variable.\n",
    "\n",
    "Let's construct a dataset with two variables, the invariant mass and lifetime, for the resonant signal defined above and the combinatorial background. To do this, we build the model in zfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:41:36.463010940Z",
     "start_time": "2023-11-09T18:41:36.367032133Z"
    }
   },
   "outputs": [],
   "source": [
    "mu = zfit.Parameter('mu', 5279, 5100, 5400)\n",
    "sigma = zfit.Parameter('sigma', 20, 1, 200)\n",
    "\n",
    "lambd = zfit.Parameter('lambda', -0.002, -0.01, 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:41:36.474956577Z",
     "start_time": "2023-11-09T18:41:36.420048637Z"
    }
   },
   "outputs": [],
   "source": [
    "obs = zfit.Space('mass', (5000, 6000))\n",
    "\n",
    "signal_pdf = zfit.pdf.Gauss(mu=mu, sigma=sigma, obs=obs)\n",
    "comb_bkg_pdf = zfit.pdf.Exponential(lambd, obs=obs)\n",
    "\n",
    "sig_yield = zfit.Parameter('sig_yield', 25000, 0, 50000,\n",
    "                                step_size=1)  # step size: default is small, use appropriate\n",
    "bkg_yield = zfit.Parameter('bkg_yield', 100000, 0, 3e5, step_size=1)\n",
    "\n",
    "# Create the extended models\n",
    "extended_sig = signal_pdf.create_extended(sig_yield)\n",
    "extended_bkg = comb_bkg_pdf.create_extended(bkg_yield)\n",
    "\n",
    "# The final model is the combination of the signal and backgrond PDF\n",
    "model = zfit.pdf.SumPDF([extended_bkg, extended_sig])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:41:39.707390072Z",
     "start_time": "2023-11-09T18:41:36.467019213Z"
    }
   },
   "outputs": [],
   "source": [
    "# Signal distributions.\n",
    "nsig_sw = 20000\n",
    "np_sig_m_sw = signal_pdf.sample(nsig_sw).numpy().reshape(-1,)\n",
    "np_sig_t_sw = np.random.exponential(size=nsig_sw, scale=1)\n",
    "\n",
    "# Background distributions.\n",
    "nbkg_sw = 150000\n",
    "np_bkg_m_sw = comb_bkg_pdf.sample(nbkg_sw).numpy().reshape(-1,)\n",
    "np_bkg_t_sw = np.random.normal(size=nbkg_sw, loc=2.0, scale=2.5)\n",
    "\n",
    "# Lifetime cut.\n",
    "t_cut = np_bkg_t_sw > 0\n",
    "np_bkg_t_sw = np_bkg_t_sw[t_cut]\n",
    "np_bkg_m_sw = np_bkg_m_sw[t_cut]\n",
    "\n",
    "# Mass distribution\n",
    "np_m_sw = np.concatenate([np_sig_m_sw, np_bkg_m_sw])\n",
    "\n",
    "# Lifetime distribution\n",
    "np_t_sw = np.concatenate([np_sig_t_sw, np_bkg_t_sw])\n",
    "\n",
    "# Plots the mass and lifetime distribution.\n",
    "fig, axs = plt.subplots(1, 2, figsize=(16, 6))\n",
    "axs[0].hist([np_bkg_m_sw, np_sig_m_sw], bins=50, stacked=True, label=(\"background\", \"signal\"), alpha=.7)\n",
    "axs[0].set_xlabel(\"m\")\n",
    "axs[0].legend(fontsize=15)\n",
    "axs[1].hist([np_bkg_t_sw, np_sig_t_sw], bins=50, stacked=True, label=(\"background\", \"signal\"), alpha=.7)\n",
    "axs[1].set_xlabel(\"t\")\n",
    "axs[1].legend(fontsize=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this particular example we want to unfold the signal lifetime distribution. To do so **sPlot** needs a discriminant variable to determine the yields of the various sources using an <ins>extended</ins> maximum likelihood fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:41:41.699026635Z",
     "start_time": "2023-11-09T18:41:39.708082190Z"
    }
   },
   "outputs": [],
   "source": [
    "# Builds the loss.\n",
    "data_sw = zfit.Data.from_numpy(obs=obs, array=np_m_sw)\n",
    "nll_sw = zfit.loss.ExtendedUnbinnedNLL(model, data_sw)\n",
    "\n",
    "# This parameter was useful in the simultaneous fit but not anymore so we fix it.\n",
    "sigma.floating = False\n",
    "\n",
    "# Minimizes the loss.\n",
    "minimizer = zfit.minimize.Minuit(use_minuit_grad=True)\n",
    "result_sw = minimizer.minimize(nll_sw)\n",
    "print(result_sw.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:41:41.701583440Z",
     "start_time": "2023-11-09T18:41:41.690949558Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_fit_projection(model, data, nbins=30, ax=None):\n",
    "    # The function will be reused.\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "\n",
    "    lower, upper = data.data_range.limit1d\n",
    "\n",
    "    # Creates and histogram of the data and plots it with mplhep.\n",
    "    counts, bin_edges = np.histogram(data.unstack_x(), bins=nbins)\n",
    "    mplhep.histplot(counts, bins=bin_edges, histtype=\"errorbar\", yerr=True,\n",
    "                    label=\"Data\", ax=ax, color=\"black\")\n",
    "\n",
    "    binwidth = np.diff(bin_edges)[0]\n",
    "    x = np.linspace(lower, upper, num=1000)  # or np.linspace\n",
    "\n",
    "    # Line plots of the total pdf and the sub-pdfs.\n",
    "    y = model.ext_pdf(x) * binwidth\n",
    "    ax.plot(x, y, label=\"total\", color=\"royalblue\")\n",
    "    for m, l, c in zip(model.get_models(), [\"background\", \"signal\"], [\"forestgreen\", \"crimson\"]):\n",
    "        ym = m.ext_pdf(x) * binwidth\n",
    "        ax.plot(x, ym, label=l, color=c)\n",
    "\n",
    "    ax.set_title(data.data_range.obs[0])\n",
    "    ax.set_xlim(lower, upper)\n",
    "    ax.legend(fontsize=15)\n",
    "\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:41:42.049462266Z",
     "start_time": "2023-11-09T18:41:41.695055246Z"
    }
   },
   "outputs": [],
   "source": [
    "# Visualization of the result.\n",
    "zfit.param.set_values(nll_sw.get_params(), result_sw)\n",
    "plot_fit_projection(model, data_sw, nbins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**sPlot** will use the fitted yield for each sources to derive the so-called **sWeights** for each data point:\n",
    "\n",
    "\\begin{equation}\n",
    "W_{n}(x) = \\frac{\\sum_{j=1}^{N_S} V_{nj} f_j(x)}{\\sum_{k=1}^{N_S} N_{k}f_k(x)}\n",
    "\\end{equation}\n",
    "\n",
    "with\n",
    "\n",
    "\\begin{equation}\n",
    "V_{nj}^{-1} = \\sum_{e=1}^{N} \\frac{f_n(x_e) f_j(x_e)}{(\\sum_{k=1}^{N_S} N_{k}f_k(x))^2}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "where ${N_S}$ is the number of sources in the data sample, here 2. The index $n$ represents the source, for instance $0$ is the signal and $1$ is the background, then $f_0$ and $N_0$ are the pdf and yield for the signal.\n",
    "\n",
    "In `hepstats` the **sWeights** are computed with the `compute_sweights` function which takes as arguments the <ins>fitted</ins> extended model and the discrimant data (on which the fit was performed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2023-11-09T18:41:42.306789925Z",
     "start_time": "2023-11-09T18:41:42.047242018Z"
    }
   },
   "outputs": [],
   "source": [
    "from hepstats.splot import compute_sweights\n",
    "\n",
    "weights = compute_sweights(model, data_sw)\n",
    "\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:41:42.308477707Z",
     "start_time": "2023-11-09T18:41:42.255058309Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Sum of signal sWeights: \", np.sum(weights[sig_yield]))\n",
    "print(\"Sum of background sWeights: \", np.sum(weights[bkg_yield]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can apply the signal **sWeights** on the lifetime distribution and retrieve its signal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:41:43.917695568Z",
     "start_time": "2023-11-09T18:41:42.302977717Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(16, 6))\n",
    "nbins = 40\n",
    "\n",
    "sorter = np_m_sw.argsort()\n",
    "\n",
    "axs[0].plot(np_m_sw[sorter], weights[sig_yield][sorter], label=\"$w_\\\\mathrm{sig}$\")\n",
    "axs[0].plot(np_m_sw[sorter], weights[bkg_yield][sorter], label=\"$w_\\\\mathrm{bkg}$\")\n",
    "axs[0].plot(np_m_sw[sorter], weights[sig_yield][sorter] + weights[bkg_yield][sorter],\n",
    "            \"-k\", label=\"$w_\\\\mathrm{sig} + w_\\\\mathrm{bkg}$\")\n",
    "axs[0].axhline(0, color=\"0.5\")\n",
    "axs[0].legend(fontsize=15)\n",
    "axs[0].set_xlim(5000, 5600)\n",
    "\n",
    "axs[1].hist(np_t_sw, bins=nbins, range=(0, 6), weights=weights[sig_yield], label=\"weighted histogram\", alpha=.5)\n",
    "axs[1].hist(np_sig_t_sw, bins=nbins, range=(0, 6), histtype=\"step\", label=\"true histogram\", lw=1.5)\n",
    "axs[1].set_xlabel(\"t\")\n",
    "axs[1].legend(fontsize=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be careful the **sPlot** technique works only on variables that are uncorrelated with the discriminant variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:41:43.935640680Z",
     "start_time": "2023-11-09T18:41:43.914957342Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Correlation between m and t: {np.corrcoef(np_m_sw, np_t_sw)[0, 1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply to signal **sWeights** on the mass distribution to see how bad the results of **sPlot** is when applied on a variable that is correlated with the discrimant variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:41:44.217521866Z",
     "start_time": "2023-11-09T18:41:43.919368369Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.hist(np_m_sw, bins=100, range=(5000, 6000), weights=weights[sig_yield]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative: Known probabilities\n",
    "\n",
    "If the probabilities are already known _beforehand_, fitting a curve with `zfit` is an extra step that is not required in order to obtain the sWeights. The next example uses `hep_ml` in order to compute the sWeights; the same function as `hepstats` also uses internally.\n",
    "\n",
    "### Building sPlot over mass\n",
    "\n",
    "Let's show how this works. First we generate two fake distributions (signal and background) with 2 variables: mass and momentum $p$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:41:44.222398796Z",
     "start_time": "2023-11-09T18:41:44.214787904Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.stats import expon, norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:41:44.615692777Z",
     "start_time": "2023-11-09T18:41:44.221274727Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=[15, 6])\n",
    "size = 10000\n",
    "\n",
    "sig_mass_distr = norm(loc=4, scale=1)\n",
    "bck_mass_distr = expon(scale=4)\n",
    "\n",
    "sig_mass = sig_mass_distr.rvs(size=size)\n",
    "bck_mass = bck_mass_distr.rvs(size=size)\n",
    "sig_p = np.random.normal(5, 1, size=size)\n",
    "bck_p = np.random.normal(3, 1, size=size)\n",
    "\n",
    "plt.subplot(121)\n",
    "\n",
    "plt.hist(sig_mass, bins=20, density=True)\n",
    "plt.hist(bck_mass, bins=20, density=True, range=(0, 10), alpha=0.5)\n",
    "plt.xlabel('mass')\n",
    "\n",
    "plt.subplot(122)\n",
    "\n",
    "plt.hist(sig_p, bins=20, density=True)\n",
    "plt.hist(bck_p, bins=20, density=True, range=(0, 10), alpha=0.5)\n",
    "plt.xlabel('p');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Of course we don't have labels which events are signal and which are background beforehand\n",
    "\n",
    "And we observe the mixture of two distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:41:44.950482870Z",
     "start_time": "2023-11-09T18:41:44.614171146Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=[15, 6])\n",
    "mass = np.concatenate([sig_mass, bck_mass])\n",
    "p = np.concatenate([sig_p, bck_p])\n",
    "\n",
    "sorter = np.argsort(mass)\n",
    "mass = mass[sorter]\n",
    "p = p[sorter]\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.hist(mass, bins=20, range=(0, 10))\n",
    "plt.xlabel('mass')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.hist(p, bins=20)\n",
    "plt.xlabel('p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have no information about real labels\n",
    "\n",
    "But we know a priori that background is distributed as exponential distribution and signal - as gaussian (more complex models can be met in practice, but idea is the same).\n",
    "\n",
    "After fitting the mixture (let me skip this process), we will get the following result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:41:45.182653107Z",
     "start_time": "2023-11-09T18:41:44.945907133Z"
    }
   },
   "outputs": [],
   "source": [
    "x = np.linspace(0, 10)\n",
    "plt.hist(mass, bins=30, range=[0, 10], density=True, alpha=0.4)\n",
    "plt.plot(x, norm.pdf(x, loc=4, scale=1) / 2., label='signal')\n",
    "plt.plot(x, expon.pdf(x, scale=4) / 2., label='bck')\n",
    "plt.plot(x, 0.5 * (norm.pdf(x, loc=4, scale=1) + expon.pdf(x, scale=4)), label='sig + bck')\n",
    "plt.legend(fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting doesn't give us information about real labels\n",
    "\n",
    "But it gives an information about probabilities, thus now we can estimate number of signal and background events within each bin.\n",
    "\n",
    "We won't use bins, but instead we will get for each event probability that it is signal or background:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:41:45.200613740Z",
     "start_time": "2023-11-09T18:41:45.185498912Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "probs = pandas.DataFrame(dict(sig=sig_mass_distr.pdf(mass), bck=bck_mass_distr.pdf(mass)))\n",
    "probs = probs.div(probs.sum(axis=1), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:41:45.535033034Z",
     "start_time": "2023-11-09T18:41:45.195431060Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(mass, probs.sig, label='sig probability')\n",
    "plt.plot(mass, probs.bck, label='bck probability')\n",
    "plt.xlim(0, 10), plt.legend(), plt.xlabel('mass')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appying sPlot\n",
    "\n",
    "sPlot converts probabilities to sWeights, using an implementation from `hep_ml`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:41:45.636452372Z",
     "start_time": "2023-11-09T18:41:45.536975210Z"
    }
   },
   "outputs": [],
   "source": [
    "from hep_ml import splot\n",
    "\n",
    "sWeights = splot.compute_sweights(probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, there are also negative sWeights, which are needed to compensate the contributions of other class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:41:46.023070838Z",
     "start_time": "2023-11-09T18:41:45.649725765Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(mass, sWeights.sig, label='sig sWeight')\n",
    "plt.plot(mass, sWeights.bck, label='bck sWeight')\n",
    "plt.xlim(0, 10), plt.legend(), plt.xlabel('mass');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using sWeights to reconstruct initial distribution\n",
    "\n",
    "Let's check that we achieved our goal and can reconstruct momentum distribution for signal and background:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:41:46.636288872Z",
     "start_time": "2023-11-09T18:41:46.023629125Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=[15, 7])\n",
    "plt.subplot(121)\n",
    "hist_conf = dict(bins=30, alpha=0.4, range=[0, 10])\n",
    "plt.hist(sig_p, label='original sig p', **hist_conf)\n",
    "plt.hist(p, weights=sWeights.sig, label='reconstructed sig p', **hist_conf)\n",
    "plt.xlabel('p'), plt.legend()\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.hist(bck_p, label='original bck p', **hist_conf)\n",
    "plt.hist(p, weights=sWeights.bck, label='reconstructed bck p', **hist_conf)\n",
    "plt.xlabel('p'), plt.legend()\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An important requirement of sPlot\n",
    "\n",
    "Reconstructed variable (i.e. $p$) and splotted variable (i.e. mass) shall be statistically independent for each class.\n",
    "\n",
    "Read the line above again. Reconstructed and splotted variable are correlated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:41:46.670488310Z",
     "start_time": "2023-11-09T18:41:46.636425810Z"
    }
   },
   "outputs": [],
   "source": [
    "np.corrcoef(abs(mass - 4), p) [0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But within each class there is no correlation, so the requirement is satisfied:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:41:46.808545533Z",
     "start_time": "2023-11-09T18:41:46.644686446Z"
    }
   },
   "outputs": [],
   "source": [
    "print(np.corrcoef(abs(sig_mass - 4), sig_p)[0, 1])\n",
    "print(np.corrcoef(abs(bck_mass - 4), bck_p)[0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as a demonstration why this is important let's use sweights to reconstruct mass (obviously mass is correlated with mass):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:41:47.029394075Z",
     "start_time": "2023-11-09T18:41:46.669469133Z"
    }
   },
   "outputs": [],
   "source": [
    "hist_conf = dict(bins=30, alpha=0.5, range=[-1, 7])\n",
    "plt.hist(sig_mass, label='original sig mass', **hist_conf)\n",
    "plt.hist(mass, weights=sWeights.sig, label='reconstructed sig mass', **hist_conf)\n",
    "plt.xlabel('mass'), plt.legend()\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\def\\ps{p_s(x)}$\n",
    "$\\def\\pb{p_b(x)}$\n",
    "$\\def\\ws{sw_s(x)}$\n",
    "$\\def\\wb{sw_b(x)}$\n",
    "\n",
    "\n",
    "\n",
    "## Derivation of sWeights (optional)\n",
    "\n",
    "Now, after we seen how this works, let's derive a formula for sWeights.\n",
    "\n",
    "\n",
    "\n",
    "The only information we have from fitting over mass is  $ \\ps $, $ \\pb$ which are probabilities of event $x$ to be signal and background.\n",
    "\n",
    "Our main goal is to correctly reconstruct histogram. Let's reconstruct the number of _signal_ events in _particular_ bin. Let's introduce unknown $p_s$ and $p_b$ - probability that signal or background event will be in the named bin.\n",
    "\n",
    "(Since mass and reconstructed variable are statistically independent for each class, $p_s$ and $p_b$ do not depend on mass.)\n",
    "\n",
    "The mathematical expectation should be obviously equal to $p_s N_s$, where $N_s$ is total amount of signal events available from fitting.\n",
    "\n",
    "Let's also introduce random variable $1_{x \\in bin}$, which is 1 iff event $x$ lies in selected bin.\n",
    "\n",
    "The __estimate for number of signal event in bin__ is equal to:\n",
    "$$X = \\sum_x \\ws \\; 1_{x \\in bin},$$ where $\\ws$ are sPlot weights and are subject to find.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This way we can guarantee that mean contribution of background are 0 (expectation is zero, but observed number will not be zero due to statistical deviation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Under assumption of linearity:\n",
    "\n",
    "*assuming* that splot weight can be computed as a linear combination of conditional probabilities:\n",
    "\n",
    "$ \\ws = a_1 \\pb + a_2 \\ps$\n",
    "\n",
    "We can easily reconstruct those numbers, first let's rewrite our system:\n",
    "\n",
    "$ \\sum_x (a_1 \\pb + a_2 \\ps) \\; \\ps = 0$ <br />\n",
    "$ \\sum_x (a_1 \\pb + a_2 \\ps) \\; \\pb = N_{sig}$\n",
    "\n",
    "$ a_1 V_{bb} + a_2 V_{bs} = 0$ <br />\n",
    "$ a_1 V_{sb} + a_2 V_{ss} = N_{sig}$ <br />\n",
    "\n",
    "Where\n",
    "$V_{ss} = \\sum_x \\ps \\; \\ps $, $V_{bs} = V_{sb} = \\sum_x \\ps \\; \\pb$, $V_{bb} = \\sum_x \\pb \\; \\pb$\n",
    "\n",
    "Having solved this linear equation, we get needed coefficients (as those in the paper)\n",
    "\n",
    "NB. There is little difference between $V$ matrix I use and $V$ matrix in the paper. <br />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimization of variation\n",
    "$\\def\\Var{\\mathbb{V}\\,}$\n",
    "\n",
    "Previous part allows one to get the correct result. But there is still no explanation of reason for linearity.\n",
    "\n",
    "\n",
    "Apart from having correct mean, we should also minimize variation of any reconstructed variable. Let's try to optimize it\n",
    "\n",
    "$$ \\Var X = \\sum_x \\ws^2 \\; \\Var 1_{x \\in bin} = \\sum_x \\ws^2 \\; (p_s \\ps + p_b \\pb)(1 - p_s \\ps - p_b \\pb)$$\n",
    "\n",
    "A bit complex, isn't it? Instead of optimizing such a complex expression (which is individual for each bin), let's minimize it's __uniform upper estimate__\n",
    "$$ \\Var X = \\sum_x \\ws^2 \\; \\Var 1_{x \\in bin} \\leq  \\sum_x \\ws^2  $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "so if we are going to minimize this upper estimate, we should solve the following optimization problem with constraints:\n",
    "<br />$\\sum_x \\ws^2 \\to \\min $\n",
    "<br />$\\sum_x \\ws \\; \\pb = 0$\n",
    "<br />$\\sum_x \\ws \\; \\ps = N_{sig}$\n",
    "\n",
    "Let's write lagrangian of optimization problem:\n",
    "<br /> $$ \\mathcal{L} =  \\sum_x \\ws^2\n",
    "+ \\lambda_1 \\left[\\sum_x \\ws \\; \\pb \\right]\n",
    "+ \\lambda_2 \\left[\\sum_x \\ws \\; \\ps - N_{sig} \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uncorrelatedness\n",
    "\n",
    "The main assuption we used here is that distribution inside each bin is abolutely indentical.\n",
    "\n",
    "In other words, we stated that there is no correlation between the index of bin and the reconstructed variable. Remember that bin corresponds to some interval in mass, and finaly we get:\n",
    "\n",
    "__reconstructed variable shall not be correlated with mass variables (or any other splotted variable)__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "1. sPlot allows reconstruction of some variables.\n",
    "2. the only information used is probabilities taken from fit over variable. If fact, any probability estimates fit well.\n",
    "3. the source of probabilities should be statistically independent from reconstructed variable (for each class!).\n",
    "4. mixture may contain more than 2 classes (this is supported by `hep_ml.splot` as well)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
